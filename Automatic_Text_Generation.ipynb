{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LCL5pcWOr_Jj",
        "outputId": "298f80f7-f0b8-4d71-9c26-9101843cf518"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "libtesseract-dev is already the newest version (4.1.1-2.1build1).\n",
            "tesseract-ocr is already the newest version (4.1.1-2.1build1).\n",
            "tesseract-ocr-rus is already the newest version (1:4.00~git30-7274cfa-1.1).\n",
            "poppler-utils is already the newest version (22.02.0-2ubuntu0.6).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 29 not upgraded.\n"
          ]
        }
      ],
      "source": [
        "!apt-get -y install poppler-utils tesseract-ocr libtesseract-dev tesseract-ocr-rus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l3uZuhM0m3w3",
        "outputId": "53864161-8f73-4cda-c546-b38bca0e3470"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: gradio 3.50.2\n",
            "Uninstalling gradio-3.50.2:\n",
            "  Successfully uninstalled gradio-3.50.2\n"
          ]
        }
      ],
      "source": [
        "!pip uninstall -y gradio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "V1W40VHi8qBo"
      },
      "outputs": [],
      "source": [
        "# Cell 3 (Modified Again)\n",
        "!pip install -q numpy ctransformers[cuda] sentence-transformers chromadb langchain langchain-community langchain-huggingface gradio==3.50.2 unstructured unstructured[pdf] unstructured[docx] python-docx fastapi uvicorn[standard] nest_asyncio pyngrok --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "8io1m_xTodiq"
      },
      "outputs": [],
      "source": [
        "# Install ctransformers with CPU support first (more reliable than GPU on some configurations)\n",
        "!pip install -q ctransformers --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "1b4KZvSE-bVW"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import gradio as gr\n",
        "import requests\n",
        "from ctransformers import AutoModelForCausalLM\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.document_loaders import DirectoryLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "\n",
        "# --- New API Imports ---\n",
        "import uvicorn\n",
        "from fastapi import FastAPI, HTTPException, Request\n",
        "from fastapi.responses import JSONResponse\n",
        "from pydantic import BaseModel\n",
        "# --- End New API Imports ---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RWE_gUQYlClK",
        "outputId": "98529c26-ca77-4011-b690-8d1acd61e755"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA available: True\n",
            "CUDA device: Tesla T4\n"
          ]
        }
      ],
      "source": [
        "# Check GPU availability\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "s3ISeNJLidf1"
      },
      "outputs": [],
      "source": [
        "# Model settings\n",
        "MODEL_URL = \"https://huggingface.co/blues-alex/YandexGPT-5-Lite-8B-pretrain-Q4_K_M-GGUF/resolve/main/yandexgpt-5-lite-8b-pretrain-q4_k_m.gguf\"\n",
        "MODEL_PATH = \"./yandexgpt-5-lite-8b-pretrain-q4_k_m.gguf\"\n",
        "RAG_DIR = \"/content/rag_db\"\n",
        "CURRENT_CONTEXT_SIZE = 2048  # Default context size\n",
        "CURRENT_GPU_LAYERS = 20      # Default GPU layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "yYCvPIOgoPlj"
      },
      "outputs": [],
      "source": [
        "# Function to download model\n",
        "def download_model(url, save_path):\n",
        "    if os.path.exists(save_path):\n",
        "        return \"Модель уже загружена\"\n",
        "    try:\n",
        "        print(f\"Загрузка модели из {url}...\")\n",
        "        response = requests.get(url, stream=True)\n",
        "        response.raise_for_status()\n",
        "        total_size = int(response.headers.get('content-length', 0))\n",
        "        block_size = 8192\n",
        "        downloaded = 0\n",
        "\n",
        "        with open(save_path, \"wb\") as f:\n",
        "            for chunk in response.iter_content(chunk_size=block_size):\n",
        "                f.write(chunk)\n",
        "                downloaded += len(chunk)\n",
        "                if total_size > 0:\n",
        "                    print(f\"\\rЗагружено: {downloaded/1024/1024:.1f}MB / {total_size/1024/1024:.1f}MB ({downloaded*100/total_size:.1f}%)\", end=\"\")\n",
        "\n",
        "        print(\"\\nМодель успешно загружена\")\n",
        "        return \"Модель успешно загружена\"\n",
        "    except requests.RequestException as e:\n",
        "        return f\"Ошибка загрузки модели: {e}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "fQOVhtF9ijSI"
      },
      "outputs": [],
      "source": [
        "# Download model if needed\n",
        "if not os.path.exists(MODEL_PATH):\n",
        "    print(\"Скачивание модели...\")\n",
        "    download_model(MODEL_URL, MODEL_PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "nzr73Z6EQOic"
      },
      "outputs": [],
      "source": [
        "# Добавьте эту функцию перед функциями update_model и change_context_size\n",
        "\n",
        "def load_model_with_params(context_size=CURRENT_CONTEXT_SIZE, gpu_layers=CURRENT_GPU_LAYERS):\n",
        "    \"\"\"\n",
        "    Загружает модель с указанными параметрами размера контекста и количества GPU слоев.\n",
        "\n",
        "    Args:\n",
        "        context_size (int): Размер контекстного окна модели\n",
        "        gpu_layers (int): Количество слоев для выполнения на GPU\n",
        "\n",
        "    Returns:\n",
        "        tuple: (model, mode) - модель и режим работы (GPU/CPU)\n",
        "    \"\"\"\n",
        "    from ctransformers import AutoModelForCausalLM\n",
        "\n",
        "    try:\n",
        "        # Try with GPU support\n",
        "        print(f\"Загрузка модели с контекстом {context_size} и {gpu_layers} GPU слоями...\")\n",
        "        model = AutoModelForCausalLM.from_pretrained(\n",
        "            MODEL_PATH,\n",
        "            model_type=\"llama\",\n",
        "            gpu_layers=gpu_layers,\n",
        "            context_length=context_size,\n",
        "            batch_size=512\n",
        "        )\n",
        "        print(f\"Модель успешно загружена с контекстом {context_size}\")\n",
        "        return model, \"GPU\"\n",
        "    except Exception as e:\n",
        "        print(f\"Ошибка при загрузке модели с GPU: {e}\")\n",
        "        print(\"Загружаем модель в режиме CPU...\")\n",
        "\n",
        "        try:\n",
        "            model = AutoModelForCausalLM.from_pretrained(\n",
        "                MODEL_PATH,\n",
        "                model_type=\"llama\",\n",
        "                gpu_layers=0,  # CPU only mode\n",
        "                context_length=context_size,\n",
        "                batch_size=512\n",
        "            )\n",
        "            print(f\"Модель загружена в режиме CPU с контекстом {context_size}\")\n",
        "            return model, \"CPU\"\n",
        "        except Exception as e:\n",
        "            raise Exception(f\"Не удалось загрузить модель: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "gcy_vARplMxc"
      },
      "outputs": [],
      "source": [
        "# Create directories\n",
        "os.makedirs(\"/content/Doc\", exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "gsC97TA2dZ-5"
      },
      "outputs": [],
      "source": [
        "# Global variable to hold the sample document creation function\n",
        "def create_sample_document():\n",
        "    \"\"\"Create a sample document for RAG testing if none exists\"\"\"\n",
        "    sample_path = \"/content/Doc/sample.docx\"\n",
        "    try:\n",
        "        from docx import Document\n",
        "        doc = Document()\n",
        "        doc.add_paragraph(\"Этот документ создан для примера работы системы RAG с УрФУ.\")\n",
        "        doc.add_paragraph(\"Уральский федеральный университет (УрФУ) расположен в Екатеринбурге.\")\n",
        "        doc.add_paragraph(\"УрФУ является одним из ведущих вузов России.\")\n",
        "        doc.add_paragraph(\"В УрФУ обучаются студенты со всей России и из многих зарубежных стран.\")\n",
        "        doc.add_paragraph(\"УрФУ предлагает программы бакалавриата, магистратуры и аспирантуры.\")\n",
        "        doc.save(sample_path)\n",
        "        print(f\"✅ Создан пример документа для тестирования: {sample_path}\")\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Ошибка при создании примера документа: {e}\")\n",
        "        return False\n",
        "\n",
        "# RAG functions with improved error handling\n",
        "def initialize_rag():\n",
        "    \"\"\"Initialize the RAG database with documents\"\"\"\n",
        "    try:\n",
        "        # Ensure the directory exists\n",
        "        os.makedirs(\"/content/Doc\", exist_ok=True)\n",
        "\n",
        "        # Check if there are any documents\n",
        "        doc_files = [f for f in os.listdir(\"/content/Doc\") if f.endswith(\".docx\")]\n",
        "\n",
        "        if not doc_files:\n",
        "            print(\"Нет документов для загрузки. Создаем пример документа...\")\n",
        "            created = create_sample_document()\n",
        "            if not created:\n",
        "                print(\"Не удалось создать пример документа. Инициализация RAG не выполнена.\")\n",
        "                return None\n",
        "\n",
        "        # Load documents\n",
        "        loader = DirectoryLoader(\"/content/Doc\", glob=\"**/*.docx\")\n",
        "        documents = loader.load()\n",
        "\n",
        "        if not documents:\n",
        "            print(\"Не удалось загрузить документы, даже после создания примера.\")\n",
        "            print(\"Проверьте, что папка /content/Doc содержит доступные файлы .docx\")\n",
        "            return None\n",
        "\n",
        "        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "        texts = text_splitter.split_documents(documents)\n",
        "\n",
        "        if not texts:\n",
        "            print(\"Документы загружены, но не удалось извлечь текст.\")\n",
        "            return None\n",
        "\n",
        "        embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\")\n",
        "        vector_db = Chroma.from_documents(texts, embeddings, persist_directory=RAG_DIR)\n",
        "        print(f\"✅ База RAG успешно инициализирована с {len(texts)} фрагментами\")\n",
        "        return vector_db\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Ошибка инициализации RAG: {e}\")\n",
        "        return None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MU84NQmGoZsl",
        "outputId": "79a7a299-2614-4d2d-a088-fae8517a0c0c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Загружаем существующую базу RAG...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "<ipython-input-13-9098590e027e>:13: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
            "  vector_db = Chroma(persist_directory=RAG_DIR, embedding_function=embeddings)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ База RAG успешно загружена\n"
          ]
        }
      ],
      "source": [
        "# Load or initialize RAG database with better error handling\n",
        "vector_db = None  # Initialize to None first\n",
        "try:\n",
        "    if not os.path.exists(RAG_DIR):\n",
        "        print(\"RAG база данных не найдена, создаем новую...\")\n",
        "        vector_db = initialize_rag()\n",
        "        if vector_db is None:\n",
        "            print(\"❌ Не удалось инициализировать RAG. Ответы модели не будут использовать контекст документов.\")\n",
        "    else:\n",
        "        try:\n",
        "            print(\"Загружаем существующую базу RAG...\")\n",
        "            embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\")\n",
        "            vector_db = Chroma(persist_directory=RAG_DIR, embedding_function=embeddings)\n",
        "            print(\"✅ База RAG успешно загружена\")\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Ошибка загрузки существующей базы RAG: {e}\")\n",
        "            print(\"Пробуем создать новую базу RAG...\")\n",
        "            if os.path.exists(RAG_DIR):\n",
        "                import shutil\n",
        "                shutil.rmtree(RAG_DIR)\n",
        "            vector_db = initialize_rag()\n",
        "            if vector_db is None:\n",
        "                print(\"❌ Не удалось инициализировать RAG. Ответы модели не будут использовать контекст документов.\")\n",
        "except Exception as e:\n",
        "    print(f\"❌ Неожиданная ошибка при работе с RAG: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "JxfLy4ntbGML"
      },
      "outputs": [],
      "source": [
        "# Фиксированный контекст, который будет передаваться модели\n",
        "FIXED_CONTEXT = \"\"\"Ты — интеллектуальный помощник, обученный отвечать на вопросы строго в рамках предоставленного контекста. Ты помощник по Уральскому Федеральному университету, который общается с пользователем. Общайся с ним только на русском языке. Объясняй максимально подробно, чтобы пользователь всё понял. Если информации недостаточно, отвечай \"Вы можете ознакомиться с данной информацией на сайте УрФУ\". Избегай выдумок и предположений. Если пользователь просит дать точные значения - обращайся только к данным RAG.\"\"\"\n",
        "CURRENT_TEMPERATURE = 0.5  # Значение по умолчанию\n",
        "# Function to generate response using RAG\n",
        "# Обновленная функция для генерации ответов с улучшенным контролем источников\n",
        "def ask_question_with_rag(question):\n",
        "    try:\n",
        "        # Check if vector_db is available\n",
        "        if vector_db is None:\n",
        "            # Fallback to model-only generation\n",
        "            prompt = f\"{FIXED_CONTEXT}\\n\\nUser: {question}\\nAssistant:\"\n",
        "            response = model(\n",
        "                prompt,\n",
        "                max_new_tokens=256,\n",
        "                temperature=CURRENT_TEMPERATURE,\n",
        "                stop=[\"User:\", \"\\n\\n\"]\n",
        "            )\n",
        "            return response.strip() + \"\\n\\n(Примечание: ответ дан без использования базы знаний, так как RAG не инициализирован)\"\n",
        "\n",
        "        # Retrieve relevant documents\n",
        "        docs = vector_db.similarity_search(question, k=3)\n",
        "\n",
        "        # Check if the retrieved documents are actually relevant by looking for keywords\n",
        "        query_keywords = set(question.lower().split())\n",
        "        important_keywords = {word for word in query_keywords\n",
        "                             if len(word) > 3 and word not in\n",
        "                             ['что', 'как', 'где', 'когда', 'какие', 'какой', 'какая', 'институт', 'урфу', 'университет', 'ИРИТ-РТФ', 'РТФ']}\n",
        "\n",
        "        # Extract and show relevant context\n",
        "        extracted_context = \"\\n\\n\".join([doc.page_content for doc in docs])\n",
        "\n",
        "        # Check if any important keywords are in the context\n",
        "        found_keywords = []\n",
        "        for keyword in important_keywords:\n",
        "            if keyword in extracted_context.lower():\n",
        "                found_keywords.append(keyword)\n",
        "\n",
        "        # Determine relevance score\n",
        "        relevance_score = len(found_keywords) / max(1, len(important_keywords)) if important_keywords else 0.5\n",
        "\n",
        "        # Add strong instruction about only using provided context\n",
        "        strict_instruction = \"\"\"\n",
        "ВАЖНО: Отвечай ТОЛЬКО на основе предоставленной информации из документов.\n",
        "Если в предоставленных документах нет ответа на вопрос, честно скажи\n",
        "\"В документах нет информации о [тема вопроса]. Вы можете ознакомиться с этой информацией на сайте УрФУ.\"\n",
        "НЕ ПРИДУМЫВАЙ информацию, которой нет в документах!\n",
        "\"\"\"\n",
        "        combined_context = f\"{FIXED_CONTEXT}\\n\\n{strict_instruction}\\n\\nДокументы:\\n{extracted_context}\"\n",
        "\n",
        "        # Create prompt for model with stronger guidance\n",
        "        prompt = f\"Context: {combined_context}\\n\\nUser: {question}\\n\\nAssistant:\"\n",
        "\n",
        "        # Use even lower temperature for low relevance scores to reduce hallucination\n",
        "        adjusted_temperature = min(CURRENT_TEMPERATURE, 0.3) if relevance_score < 0.5 else CURRENT_TEMPERATURE\n",
        "\n",
        "        # Generate response with ctransformers\n",
        "        response = model(\n",
        "            prompt,\n",
        "            max_new_tokens=256,\n",
        "            temperature=adjusted_temperature,  # Use adjusted temperature based on relevance\n",
        "            stop=[\"User:\", \"\\n\\n\"]\n",
        "        )\n",
        "\n",
        "        # Return the generated text with optional debugging info\n",
        "        result = response.strip()\n",
        "\n",
        "        # For debugging - uncomment to show relevance information\n",
        "        # debug_info = f\"\\n\\n[Отладка: Найдено {len(found_keywords)}/{len(important_keywords)} ключевых слов, релевантность {relevance_score:.2f}]\"\n",
        "        # return result + debug_info\n",
        "\n",
        "        return result\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"Произошла ошибка при генерации ответа: {str(e)}\\n\\nПожалуйста, попробуйте очистить и переинициализировать RAG.\"\n",
        "\n",
        "# Function to update model\n",
        "def update_model(link):\n",
        "    try:\n",
        "        global model, mode\n",
        "        result = download_model(link, MODEL_PATH)\n",
        "        # Reload with current context size\n",
        "        model, mode = load_model_with_params(CURRENT_CONTEXT_SIZE, CURRENT_GPU_LAYERS)\n",
        "        return f\"Модель обновлена: {result}. Режим работы: {mode}\"\n",
        "    except Exception as e:\n",
        "        return f\"Ошибка при обновлении модели: {str(e)}\"\n",
        "\n",
        "# Function to change context size\n",
        "def change_context_size(new_size_str):\n",
        "    try:\n",
        "        global model, CURRENT_CONTEXT_SIZE, mode\n",
        "\n",
        "        # Convert to integer and validate\n",
        "        new_size = int(new_size_str)\n",
        "        if new_size < 512:\n",
        "            return \"Ошибка: размер контекста должен быть не менее 512\"\n",
        "        if new_size > 8192:\n",
        "            return \"Ошибка: размер контекста не может превышать 8192\"\n",
        "\n",
        "        CURRENT_CONTEXT_SIZE = new_size\n",
        "        model, mode = load_model_with_params(CURRENT_CONTEXT_SIZE, CURRENT_GPU_LAYERS)\n",
        "        return f\"Размер контекста изменен на {new_size}. Режим работы: {mode}\"\n",
        "    except ValueError:\n",
        "        return \"Ошибка: введите корректное целое число\"\n",
        "    except Exception as e:\n",
        "        return f\"Ошибка при изменении размера контекста: {str(e)}\"\n",
        "\n",
        "def change_temperature(new_temp_str):\n",
        "    try:\n",
        "        global CURRENT_TEMPERATURE\n",
        "\n",
        "        # Convert to float and validate\n",
        "        new_temp = float(new_temp_str)\n",
        "        if new_temp < 0.0:\n",
        "            return \"Ошибка: температура не может быть меньше 0.0\"\n",
        "        if new_temp > 2.0:\n",
        "            return \"Ошибка: температура не рекомендуется выше 2.0\"\n",
        "\n",
        "        CURRENT_TEMPERATURE = new_temp\n",
        "        return f\"Температура генерации изменена на {new_temp}\"\n",
        "    except ValueError:\n",
        "        return \"Ошибка: введите корректное число с плавающей точкой (например, 0.2)\"\n",
        "    except Exception as e:\n",
        "        return f\"Ошибка при изменении температуры: {str(e)}\"\n",
        "\n",
        "# Function to clear RAG\n",
        "def clear_rag():\n",
        "    try:\n",
        "        if os.path.exists(RAG_DIR):\n",
        "            import shutil\n",
        "            shutil.rmtree(RAG_DIR)\n",
        "        global vector_db\n",
        "        vector_db = initialize_rag()\n",
        "        return \"RAG очищен и переинициализирован\"\n",
        "    except Exception as e:\n",
        "        return f\"Ошибка при очистке RAG: {str(e)}\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 14.1 (New) - API Models\n",
        "class QuestionRequest(BaseModel):\n",
        "    question: str\n",
        "\n",
        "class AnswerResponse(BaseModel):\n",
        "    answer: str"
      ],
      "metadata": {
        "id": "fTOCtgJu4vCO"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 14.5 (New) - Install cloudflared\n",
        "print(\"Установка cloudflared...\")\n",
        "!wget -q https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64 -O cloudflared\n",
        "!chmod +x cloudflared\n",
        "print(\"cloudflared установлен.\")\n",
        "# Переместим в /usr/local/bin для удобства вызова (не обязательно, но рекомендуется)\n",
        "!mv cloudflared /usr/local/bin/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TjtfQVuu8J3t",
        "outputId": "2ff904c5-7c2d-4eca-fad5-b2415a21e5b3"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Установка cloudflared...\n",
            "cloudflared установлен.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NIBY23V2CIZk",
        "outputId": "dc3faaaa-b2a1-45db-c9bb-ca47db156d89"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Загрузка модели...\n",
            "Загрузка модели с контекстом 2048 и 20 GPU слоями...\n",
            "Модель успешно загружена с контекстом 2048\n",
            "Модель загружена. Режим: GPU\n",
            "Определение интерфейса Gradio (Чат + Админ-панель)...\n",
            "Интерфейс Gradio определен.\n",
            "Создание FastAPI приложения...\n",
            "FastAPI приложение создано.\n",
            "API эндпоинт /api/ask определен.\n",
            "Монтирование Gradio UI на FastAPI...\n",
            "Gradio UI смонтирован на FastAPI по пути '/'.\n",
            "Запуск асинхронной функции для сервера и туннеля...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:     Started server process [4977]\n",
            "INFO:     Waiting for application startup.\n",
            "INFO:     Application startup complete.\n",
            "INFO:     Uvicorn running on http://0.0.0.0:7860 (Press CTRL+C to quit)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Сервер Uvicorn запущен в фоновом режиме.\n",
            "IMPORTANT: You are using gradio version 3.50.2, however version 4.44.1 is available, please upgrade.\n",
            "--------\n",
            "Запуск cloudflared...\n",
            "[cloudflared] 2025-03-26T10:37:19Z INF Thank you for trying Cloudflare Tunnel. Doing so, without a Cloudflare account, is a quick way to experiment and try it out. However, be aware that these account-less Tunnels have no uptime guarantee, are subject to the Cloudflare Online Services Terms of Use (https://www.cloudflare.com/website-terms/), and Cloudflare reserves the right to investigate your use of Tunnels for violations of such terms. If you intend to use Tunnels in production you should use a pre-created named tunnel by following: https://developers.cloudflare.com/cloudflare-one/connections/connect-apps\n",
            "[cloudflared] 2025-03-26T10:37:19Z INF Requesting new quick Tunnel on trycloudflare.com...\n",
            "[cloudflared] 2025-03-26T10:37:23Z INF +--------------------------------------------------------------------------------------------+\n",
            "[cloudflared] 2025-03-26T10:37:23Z INF |  Your quick Tunnel has been created! Visit it at (it may take some time to be reachable):  |\n",
            "[cloudflared] 2025-03-26T10:37:23Z INF |  https://out-guidance-unnecessary-furniture.trycloudflare.com                              |\n",
            "\n",
            "==================================================\n",
            "✅ Публичный URL (Cloudflare Tunnel): https://out-guidance-unnecessary-furniture.trycloudflare.com\n",
            "   API: https://out-guidance-unnecessary-furniture.trycloudflare.com/api/ask | Docs: https://out-guidance-unnecessary-furniture.trycloudflare.com/docs\n",
            "--- Сервер и туннель активны. Остановите ячейку для завершения. ---\n",
            "==================================================\n",
            "\n",
            "INFO:     195.181.175.171:0 - \"GET / HTTP/1.1\" 200 OK\n",
            "INFO:     195.181.175.171:0 - \"GET /assets/index-a959df42.js HTTP/1.1\" 200 OK\n",
            "INFO:     195.181.175.171:0 - \"GET /assets/index-5ac12191.css HTTP/1.1\" 200 OK\n",
            "INFO:     195.181.175.171:0 - \"GET /info HTTP/1.1\" 200 OK\n",
            "INFO:     195.181.175.171:0 - \"GET /favicon.ico HTTP/1.1\" 200 OK\n",
            "INFO:     195.181.175.171:0 - \"GET /theme.css HTTP/1.1\" 200 OK\n",
            "INFO:     195.181.175.171:0 - \"GET /assets/Blocks-0733f3b3.css HTTP/1.1\" 200 OK\n",
            "INFO:     195.181.175.171:0 - \"GET /assets/Button-dd3ccceb.css HTTP/1.1\" 200 OK\n",
            "INFO:     195.181.175.171:0 - \"GET /assets/Blocks-14eb0294.js HTTP/1.1\" 200 OK\n",
            "INFO:     195.181.175.171:0 - \"GET /assets/Button-fb0eed2f.js HTTP/1.1\" 200 OK\n",
            "INFO:     195.181.175.171:0 - \"GET /assets/index-93c91554.css HTTP/1.1\" 200 OK\n",
            "INFO:     195.181.175.171:0 - \"GET /assets/index-a5fdef40.js HTTP/1.1\" 200 OK\n",
            "INFO:     195.181.175.171:0 - \"GET /assets/Info-91922efa.js HTTP/1.1\" 200 OK\n",
            "INFO:     195.181.175.171:0 - \"GET /assets/InteractiveTextbox-8c4d1407.js HTTP/1.1\" 200 OK\n",
            "INFO:     195.181.175.171:0 - \"GET /assets/StaticForm-3812b7f1.css HTTP/1.1\" 200 OK\n",
            "INFO:     195.181.175.171:0 - \"GET /assets/StaticForm-0d359fd6.js HTTP/1.1\" 200 OK\n",
            "INFO:     195.181.175.171:0 - \"GET /assets/StaticColumn-e8db0143.js HTTP/1.1\" 200 OK\n",
            "INFO:     195.181.175.171:0 - \"GET /assets/Textbox-8e45e7cb.js HTTP/1.1\" 200 OK\n",
            "INFO:     195.181.175.171:0 - \"GET /assets/index-e5090eed.js HTTP/1.1\" 200 OK\n",
            "INFO:     195.181.175.171:0 - \"GET /assets/Copy-76383fd2.js HTTP/1.1\" 200 OK\n",
            "INFO:     195.181.175.171:0 - \"GET /assets/index-6bb91bb9.js HTTP/1.1\" 200 OK\n",
            "INFO:     195.181.175.171:0 - \"GET /assets/utils-c3e3db58.js HTTP/1.1\" 200 OK\n",
            "INFO:     195.181.175.171:0 - \"GET /assets/index-4594c4cd.js HTTP/1.1\" 200 OK\n",
            "INFO:     195.181.175.171:0 - \"GET /assets/api-logo-5346f193.svg HTTP/1.1\" 200 OK\n",
            "INFO:     195.181.175.171:0 - \"GET /assets/Textbox-dde6f8cc.css HTTP/1.1\" 200 OK\n",
            "INFO:     195.181.175.171:0 - \"GET /assets/StaticColumn-2853eb31.css HTTP/1.1\" 200 OK\n",
            "INFO:     195.181.175.171:0 - \"GET /assets/index-e75b4bc3.js HTTP/1.1\" 200 OK\n",
            "INFO:     195.181.175.171:0 - \"GET /assets/index-2c0d26e2.js HTTP/1.1\" 200 OK\n",
            "INFO:     195.181.175.171:0 - \"GET /assets/logo-0a070fcf.svg HTTP/1.1\" 200 OK\n",
            "INFO:     195.181.175.171:0 - \"GET /assets/StaticMarkdown-06312fb7.js HTTP/1.1\" 200 OK\n",
            "INFO:     195.181.175.171:0 - \"GET /assets/BlockTitle-a5d1fe95.js HTTP/1.1\" 200 OK\n",
            "INFO:     195.181.175.171:0 - \"GET /assets/index-31fa361b.js HTTP/1.1\" 200 OK\n",
            "INFO:     195.181.175.171:0 - \"GET /assets/StaticMarkdown-1e08a987.css HTTP/1.1\" 200 OK\n",
            "INFO:     34.83.203.92:0 - \"GET / HTTP/1.1\" 200 OK\n",
            "INFO:     34.83.203.92:0 - \"GET / HTTP/1.1\" 200 OK\n",
            "INFO:     34.83.203.92:0 - \"GET / HTTP/1.1\" 200 OK\n",
            "INFO:     34.83.203.92:0 - \"GET / HTTP/1.1\" 200 OK\n",
            "INFO:     34.83.203.92:0 - \"GET /assets/index-a959df42.js HTTP/1.1\" 200 OK\n",
            "INFO:     34.83.203.92:0 - \"GET /assets/index-a959df42.js HTTP/1.1\" 200 OK\n",
            "INFO:     34.83.203.92:0 - \"GET /assets/index-5ac12191.css HTTP/1.1\" 200 OK\n",
            "INFO:     34.83.203.92:0 - \"GET /info HTTP/1.1\" 200 OK\n",
            "INFO:     34.83.203.92:0 - \"GET /assets/index-5ac12191.css HTTP/1.1\" 200 OK\n",
            "INFO:     34.83.203.92:0 - \"GET /theme.css HTTP/1.1\" 200 OK\n",
            "INFO:     34.83.203.92:0 - \"GET /assets/Button-dd3ccceb.css HTTP/1.1\" 200 OK\n",
            "INFO:     34.83.203.92:0 - \"GET /assets/Blocks-14eb0294.js HTTP/1.1\" 200 OK\n",
            "INFO:     34.83.203.92:0 - \"GET /assets/Blocks-0733f3b3.css HTTP/1.1\" 200 OK\n",
            "INFO:     34.83.203.92:0 - \"GET /assets/Button-fb0eed2f.js HTTP/1.1\" 200 OK\n",
            "INFO:     34.83.203.92:0 - \"GET /favicon.ico HTTP/1.1\" 200 OK\n",
            "INFO:     34.83.203.92:0 - \"GET /info HTTP/1.1\" 200 OK\n",
            "INFO:     34.83.203.92:0 - \"GET /assets/BlockTitle-a5d1fe95.js HTTP/1.1\" 200 OK\n",
            "INFO:     34.83.203.92:0 - \"GET /assets/index-a5fdef40.js HTTP/1.1\" 200 OK\n",
            "INFO:     34.83.203.92:0 - \"GET /assets/StaticColumn-e8db0143.js HTTP/1.1\" 200 OK\n",
            "INFO:     34.83.203.92:0 - \"GET /assets/Info-91922efa.js HTTP/1.1\" 200 OK\n",
            "INFO:     34.83.203.92:0 - \"GET /assets/InteractiveTextbox-8c4d1407.js HTTP/1.1\" 200 OK\n",
            "INFO:     34.83.203.92:0 - \"GET /assets/StaticForm-0d359fd6.js HTTP/1.1\" 200 OK\n",
            "INFO:     34.83.203.92:0 - \"GET /assets/Button-dd3ccceb.css HTTP/1.1\" 200 OK\n",
            "INFO:     34.83.203.92:0 - \"GET /assets/index-2c0d26e2.js HTTP/1.1\" 200 OK\n",
            "INFO:     34.83.203.92:0 - \"GET /assets/StaticMarkdown-1e08a987.css HTTP/1.1\" 200 OK\n",
            "INFO:     34.83.203.92:0 - \"GET /assets/Textbox-dde6f8cc.css HTTP/1.1\" 200 OK\n",
            "INFO:     34.83.203.92:0 - \"GET /assets/utils-c3e3db58.js HTTP/1.1\" 200 OK\n",
            "INFO:     34.83.203.92:0 - \"GET /assets/index-93c91554.css HTTP/1.1\" 200 OK\n",
            "INFO:     34.83.203.92:0 - \"GET /assets/index-4594c4cd.js HTTP/1.1\" 200 OK\n",
            "INFO:     34.83.203.92:0 - \"GET /assets/index-6bb91bb9.js HTTP/1.1\" 200 OK\n",
            "INFO:     34.83.203.92:0 - \"GET /assets/StaticForm-3812b7f1.css HTTP/1.1\" 200 OK\n",
            "INFO:     34.83.203.92:0 - \"GET /assets/StaticColumn-2853eb31.css HTTP/1.1\" 200 OK\n",
            "INFO:     34.83.203.92:0 - \"GET /assets/index-31fa361b.js HTTP/1.1\" 200 OK\n",
            "INFO:     34.83.203.92:0 - \"GET /assets/Copy-76383fd2.js HTTP/1.1\" 200 OK\n",
            "INFO:     34.83.203.92:0 - \"GET /assets/StaticMarkdown-06312fb7.js HTTP/1.1\" 200 OK\n",
            "INFO:     34.83.203.92:0 - \"GET /assets/api-logo-5346f193.svg HTTP/1.1\" 200 OK\n",
            "INFO:     34.83.203.92:0 - \"GET /assets/Textbox-8e45e7cb.js HTTP/1.1\" 200 OK\n",
            "INFO:     34.83.203.92:0 - \"GET /assets/logo-0a070fcf.svg HTTP/1.1\" 200 OK\n",
            "INFO:     34.83.203.92:0 - \"GET /assets/index-e75b4bc3.js HTTP/1.1\" 200 OK\n",
            "INFO:     34.83.203.92:0 - \"GET /assets/index-e5090eed.js HTTP/1.1\" 200 OK\n",
            "INFO:     34.83.203.92:0 - \"GET /theme.css HTTP/1.1\" 200 OK\n",
            "INFO:     34.83.203.92:0 - \"GET /assets/Button-fb0eed2f.js HTTP/1.1\" 200 OK\n",
            "INFO:     34.83.203.92:0 - \"GET /assets/Blocks-0733f3b3.css HTTP/1.1\" 200 OK\n",
            "INFO:     34.83.203.92:0 - \"GET /assets/Blocks-14eb0294.js HTTP/1.1\" 200 OK\n",
            "INFO:     34.83.203.92:0 - \"GET /assets/Textbox-dde6f8cc.css HTTP/1.1\" 200 OK\n",
            "INFO:     34.83.203.92:0 - \"GET /assets/index-93c91554.css HTTP/1.1\" 200 OK\n",
            "INFO:     34.83.203.92:0 - \"GET /assets/StaticColumn-2853eb31.css HTTP/1.1\" 200 OK\n",
            "INFO:     34.83.203.92:0 - \"GET /assets/index-2c0d26e2.js HTTP/1.1\" 200 OK\n",
            "INFO:     34.83.203.92:0 - \"GET /assets/StaticMarkdown-1e08a987.css HTTP/1.1\" 200 OK\n",
            "INFO:     34.83.203.92:0 - \"GET /assets/index-31fa361b.js HTTP/1.1\" 200 OK\n",
            "INFO:     34.83.203.92:0 - \"GET /assets/StaticMarkdown-06312fb7.js HTTP/1.1\" 200 OK\n",
            "INFO:     34.83.203.92:0 - \"GET /assets/StaticForm-0d359fd6.js HTTP/1.1\" 200 OK\n",
            "INFO:     34.83.203.92:0 - \"GET /assets/utils-c3e3db58.js HTTP/1.1\" 200 OK\n",
            "INFO:     34.83.203.92:0 - \"GET /assets/InteractiveTextbox-8c4d1407.js HTTP/1.1\" 200 OK\n",
            "INFO:     34.83.203.92:0 - \"GET /assets/index-a5fdef40.js HTTP/1.1\" 200 OK\n",
            "INFO:     34.83.203.92:0 - \"GET /assets/api-logo-5346f193.svg HTTP/1.1\" 200 OK\n",
            "INFO:     34.83.203.92:0 - \"GET /assets/index-4594c4cd.js HTTP/1.1\" 200 OK\n",
            "INFO:     34.83.203.92:0 - \"GET /assets/index-6bb91bb9.js HTTP/1.1\" 200 OK\n",
            "INFO:     34.83.203.92:0 - \"GET /assets/BlockTitle-a5d1fe95.js HTTP/1.1\" 200 OK\n",
            "INFO:     34.83.203.92:0 - \"GET /assets/StaticForm-3812b7f1.css HTTP/1.1\" 200 OK\n",
            "INFO:     34.83.203.92:0 - \"GET /assets/Textbox-8e45e7cb.js HTTP/1.1\" 200 OK\n",
            "INFO:     34.83.203.92:0 - \"GET /assets/Copy-76383fd2.js HTTP/1.1\" 200 OK\n",
            "INFO:     34.83.203.92:0 - \"GET /assets/StaticColumn-e8db0143.js HTTP/1.1\" 200 OK\n",
            "INFO:     34.83.203.92:0 - \"GET /assets/Info-91922efa.js HTTP/1.1\" 200 OK\n",
            "INFO:     34.83.203.92:0 - \"GET /assets/index-e75b4bc3.js HTTP/1.1\" 200 OK\n",
            "INFO:     34.83.203.92:0 - \"GET /assets/index-e5090eed.js HTTP/1.1\" 200 OK\n",
            "INFO:     34.83.203.92:0 - \"GET /assets/logo-0a070fcf.svg HTTP/1.1\" 200 OK\n",
            "INFO:     195.181.175.171:0 - \"POST /run/predict HTTP/1.1\" 200 OK\n",
            "INFO:     195.181.175.171:0 - \"POST /run/predict HTTP/1.1\" 200 OK\n",
            "INFO:     195.181.175.171:0 - \"POST /run/predict HTTP/1.1\" 200 OK\n",
            "Загрузка модели с контекстом 4096 и 20 GPU слоями...\n",
            "Модель успешно загружена с контекстом 4096\n",
            "INFO:     195.181.175.171:0 - \"POST /run/predict HTTP/1.1\" 200 OK\n",
            "INFO:     195.181.175.171:0 - \"POST /run/predict HTTP/1.1\" 200 OK\n",
            "INFO:     195.181.175.171:0 - \"GET /docs HTTP/1.1\" 200 OK\n",
            "INFO:     195.181.175.171:0 - \"GET /openapi.json HTTP/1.1\" 200 OK\n",
            "[API Request] /api/ask - Вопрос: Что это за институт?\n",
            "[API Response] /api/ask - Ответ сгенерирован.\n",
            "INFO:     195.181.175.171:0 - \"POST /api/ask HTTP/1.1\" 200 OK\n",
            "[API Request] /api/ask - Вопрос: Что это за институт?\n",
            "[API Response] /api/ask - Ответ сгенерирован.\n",
            "INFO:     195.181.175.171:0 - \"POST /api/ask HTTP/1.1\" 200 OK\n"
          ]
        }
      ],
      "source": [
        "# Cell 15 (Возвращена Админ-Панель, БЕЗ Whisper, с Cloudflared)\n",
        "\n",
        "import asyncio\n",
        "import nest_asyncio\n",
        "import uvicorn\n",
        "import os\n",
        "import subprocess\n",
        "import re\n",
        "import time\n",
        "from pydantic import BaseModel # Убедимся, что импортирован\n",
        "from fastapi import FastAPI, HTTPException # Убедимся, что импортированы\n",
        "import gradio as gr # Убедимся, что импортирован\n",
        "\n",
        "# --- API Models (если еще не определены выше) ---\n",
        "class QuestionRequest(BaseModel):\n",
        "    question: str\n",
        "\n",
        "class AnswerResponse(BaseModel):\n",
        "    answer: str\n",
        "\n",
        "# --- 1. Load Model and Initialize RAG (as before) ---\n",
        "print(\"Загрузка модели...\")\n",
        "model, mode = load_model_with_params(CURRENT_CONTEXT_SIZE, CURRENT_GPU_LAYERS)\n",
        "print(f\"Модель загружена. Режим: {mode}\")\n",
        "# Убедитесь, что RAG инициализирован (vector_db существует)\n",
        "\n",
        "# --- 2. Define Gradio UI (Чат + ВОЗВРАЩЕННАЯ АДМИН-ПАНЕЛЬ) ---\n",
        "print(\"Определение интерфейса Gradio (Чат + Админ-панель)...\")\n",
        "with gr.Blocks() as demo:\n",
        "    # --- Чат ---\n",
        "    with gr.Row():\n",
        "        gr.Markdown(\"### Чат с AI\")\n",
        "    chat_input = gr.Textbox(label=\"Введите вопрос\")\n",
        "    chat_output = gr.Textbox(label=\"Ответ\", lines=10, interactive=False) # Output non-interactive\n",
        "    submit_button = gr.Button(\"Отправить\")\n",
        "    # Связываем кнопку отправки чата\n",
        "    submit_button.click(ask_question_with_rag, inputs=chat_input, outputs=chat_output)\n",
        "\n",
        "    # --- ВОЗВРАЩЕННАЯ АДМИН-ПАНЕЛЬ ---\n",
        "    with gr.Row():\n",
        "        gr.Markdown(\"### Админ-панель\")\n",
        "\n",
        "    with gr.Row():\n",
        "        # Колонка обновления модели\n",
        "        with gr.Column():\n",
        "            model_link = gr.Textbox(label=\"Ссылка на модель (GGUF)\")\n",
        "            update_model_button = gr.Button(\"Обновить модель\")\n",
        "            update_model_output = gr.Textbox(label=\"Статус обновления модели\", interactive=False)\n",
        "            # Связываем кнопку обновления модели\n",
        "            update_model_button.click(update_model, inputs=model_link, outputs=update_model_output)\n",
        "\n",
        "        # Колонка изменения размера контекста\n",
        "        with gr.Column():\n",
        "            context_size_input = gr.Textbox(label=\"Размер контекста (512-8192)\", value=str(CURRENT_CONTEXT_SIZE))\n",
        "            context_size_button = gr.Button(\"Изменить размер контекста\")\n",
        "            context_size_output = gr.Textbox(label=\"Статус изменения контекста\", interactive=False)\n",
        "            # Связываем кнопку изменения контекста\n",
        "            context_size_button.click(change_context_size, inputs=context_size_input, outputs=context_size_output)\n",
        "\n",
        "    with gr.Row():\n",
        "        # Колонка изменения температуры\n",
        "        with gr.Column():\n",
        "            temperature_input = gr.Textbox(\n",
        "                label=\"Температура (0.0-2.0)\",\n",
        "                value=str(CURRENT_TEMPERATURE)\n",
        "            )\n",
        "            temperature_button = gr.Button(\"Изменить температуру\")\n",
        "            temperature_output = gr.Textbox(label=\"Статус изменения температуры\", interactive=False)\n",
        "            # Связываем кнопку изменения температуры\n",
        "            temperature_button.click(change_temperature, inputs=temperature_input, outputs=temperature_output)\n",
        "\n",
        "        # Колонка очистки RAG\n",
        "        with gr.Column():\n",
        "            rag_button = gr.Button(\"Очистить и переинициализировать RAG\")\n",
        "            rag_output = gr.Textbox(label=\"Статус RAG\", interactive=False)\n",
        "            # Связываем кнопку очистки RAG\n",
        "            rag_button.click(clear_rag, outputs=rag_output)\n",
        "\n",
        "print(\"Интерфейс Gradio определен.\")\n",
        "\n",
        "# --- 3. Create FastAPI App ---\n",
        "print(\"Создание FastAPI приложения...\")\n",
        "api_app = FastAPI(title=\"Text Generation API\", description=\"API для генерации текста с использованием RAG\")\n",
        "print(\"FastAPI приложение создано.\")\n",
        "\n",
        "# --- 4. Define API Endpoint (/api/ask) ---\n",
        "@api_app.post(\"/api/ask\",\n",
        "              response_model=AnswerResponse,\n",
        "              summary=\"Задать вопрос модели\",\n",
        "              description=\"Отправляет вопрос модели (с использованием RAG, если доступно) и возвращает ответ.\")\n",
        "async def handle_ask_api(request: QuestionRequest):\n",
        "    print(f\"[API Request] /api/ask - Вопрос: {request.question}\")\n",
        "    try:\n",
        "        answer = ask_question_with_rag(request.question)\n",
        "        print(f\"[API Response] /api/ask - Ответ сгенерирован.\")\n",
        "        return AnswerResponse(answer=answer)\n",
        "    except Exception as e:\n",
        "        print(f\"[API Error] /api/ask - Ошибка: {str(e)}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        raise HTTPException(status_code=500, detail=f\"Ошибка генерации ответа: {str(e)}\")\n",
        "print(\"API эндпоинт /api/ask определен.\")\n",
        "\n",
        "# --- 5. Mount Gradio App onto FastAPI ---\n",
        "print(\"Монтирование Gradio UI на FastAPI...\")\n",
        "# Убедитесь, что 'demo' - это ваш объект gr.Blocks()\n",
        "api_app = gr.mount_gradio_app(api_app, demo, path=\"/\")\n",
        "print(\"Gradio UI смонтирован на FastAPI по пути '/'.\")\n",
        "\n",
        "# --- 6. Run with Uvicorn and Cloudflared Tunnel ---\n",
        "nest_asyncio.apply()\n",
        "\n",
        "async def run_server_and_tunnel():\n",
        "    config = uvicorn.Config(app=api_app, host=\"0.0.0.0\", port=7860, log_level=\"info\")\n",
        "    server = uvicorn.Server(config)\n",
        "    server_task = asyncio.create_task(server.serve())\n",
        "    print(\"Сервер Uvicorn запущен в фоновом режиме.\")\n",
        "    await asyncio.sleep(5)\n",
        "\n",
        "    print(\"Запуск cloudflared...\")\n",
        "    cf_process = subprocess.Popen(\n",
        "        ['cloudflared', 'tunnel', '--url', 'http://localhost:7860'],\n",
        "        stdout=subprocess.PIPE, stderr=subprocess.PIPE, universal_newlines=True\n",
        "    )\n",
        "\n",
        "    public_url = None\n",
        "    login_url_printed = False\n",
        "    start_time = time.time()\n",
        "    try:\n",
        "        while time.time() - start_time < 60:\n",
        "            line = cf_process.stderr.readline()\n",
        "            if not line and cf_process.poll() is not None: break\n",
        "            if not line:\n",
        "                await asyncio.sleep(0.5); continue\n",
        "            print(f\"[cloudflared] {line.strip()}\")\n",
        "\n",
        "            # Логин URL (первый раз)\n",
        "            if \"https://dash.cloudflare.com/argotunnel?callback=\" in line:\n",
        "                 login_url = re.search(r'(https://dash.cloudflare.com/argotunnel\\?callback=[^\\s]+)', line)\n",
        "                 if login_url:\n",
        "                      print(\"\\n\" + \"=\"*50 + \"\\n‼️ ТРЕБУЕТСЯ АУТЕНТИФИКАЦИЯ CLOUDFLARE (ОДИН РАЗ) ‼️\")\n",
        "                      print(f\"1. Скопируйте ссылку:\\n    {login_url.group(1)}\\n\")\n",
        "                      print(\"2. Откройте в браузере, войдите в Cloudflare, авторизуйте.\")\n",
        "                      print(\"3. После успеха в браузере, ОСТАНОВИТЕ и ПЕРЕЗАПУСТИТЕ эту ячейку.\\n\" + \"=\"*50 + \"\\n\")\n",
        "                      login_url_printed = True; cf_process.terminate(); await cf_process.wait(); return\n",
        "\n",
        "            # Готовый URL (после логина)\n",
        "            tunnel_url_match = re.search(r'(https://[a-zA-Z0-9-]+\\.trycloudflare\\.com)', line)\n",
        "            if tunnel_url_match:\n",
        "                public_url = tunnel_url_match.group(1)\n",
        "                print(\"\\n\" + \"=\"*50)\n",
        "                print(f\"✅ Публичный URL (Cloudflare Tunnel): {public_url}\")\n",
        "                print(f\"   API: {public_url}/api/ask | Docs: {public_url}/docs\")\n",
        "                print(\"--- Сервер и туннель активны. Остановите ячейку для завершения. ---\\n\" + \"=\"*50 + \"\\n\")\n",
        "                break # URL найден\n",
        "\n",
        "        if not public_url and not login_url_printed:\n",
        "             print(\"Не удалось получить URL от cloudflared за 60 секунд.\"); cf_process.terminate(); await cf_process.wait(); return\n",
        "\n",
        "        await server_task # Ждем завершения Uvicorn\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Ошибка: {e}\"); server_task.cancel()\n",
        "        try: await server_task\n",
        "        except asyncio.CancelledError: print(\"Сервер Uvicorn остановлен.\")\n",
        "    finally:\n",
        "        print(\"Завершение cloudflared...\");\n",
        "        if cf_process.poll() is None: # Проверяем, жив ли еще процесс\n",
        "             cf_process.terminate()\n",
        "             try:\n",
        "                 await asyncio.wait_for(asyncio.to_thread(cf_process.wait), timeout=5.0)\n",
        "             except asyncio.TimeoutError:\n",
        "                 print(\"Процесс cloudflared не завершился вовремя, убиваем...\")\n",
        "                 cf_process.kill()\n",
        "                 await asyncio.to_thread(cf_process.wait)\n",
        "        print(\"Процесс cloudflared завершен.\")\n",
        "\n",
        "\n",
        "# --- Запуск ---\n",
        "print(\"Запуск асинхронной функции для сервера и туннеля...\")\n",
        "asyncio.run(run_server_and_tunnel())"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}