{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!apt-get -y install poppler-utils tesseract-ocr libtesseract-dev tesseract-ocr-rus"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LCL5pcWOr_Jj",
        "outputId": "3d385bab-30b4-4f05-e599-5660e63b2d35"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "libtesseract-dev is already the newest version (4.1.1-2.1build1).\n",
            "tesseract-ocr is already the newest version (4.1.1-2.1build1).\n",
            "tesseract-ocr-rus is already the newest version (1:4.00~git30-7274cfa-1.1).\n",
            "poppler-utils is already the newest version (22.02.0-2ubuntu0.6).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 29 not upgraded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall -y gradio"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l3uZuhM0m3w3",
        "outputId": "5013fef5-a372-46d2-c568-2fb5a36d3207"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: gradio 3.50.2\n",
            "Uninstalling gradio-3.50.2:\n",
            "  Successfully uninstalled gradio-3.50.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "V1W40VHi8qBo"
      },
      "outputs": [],
      "source": [
        "!pip install -q numpy ctransformers[cuda] sentence-transformers chromadb langchain langchain-community langchain-huggingface gradio==3.50.2 unstructured unstructured[pdf] unstructured[docx] python-docx --quiet\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install ctransformers with CPU support first (more reliable than GPU on some configurations)\n",
        "!pip install -q ctransformers --quiet"
      ],
      "metadata": {
        "id": "8io1m_xTodiq"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import gradio as gr\n",
        "import requests\n",
        "from ctransformers import AutoModelForCausalLM\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.document_loaders import DirectoryLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_huggingface import HuggingFaceEmbeddings"
      ],
      "metadata": {
        "id": "1b4KZvSE-bVW"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check GPU availability\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RWE_gUQYlClK",
        "outputId": "d014cf2c-6885-40e2-bce0-69dbdc1c007c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA available: True\n",
            "CUDA device: Tesla T4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Model settings\n",
        "MODEL_URL = \"https://huggingface.co/blues-alex/YandexGPT-5-Lite-8B-pretrain-Q4_K_M-GGUF/resolve/main/yandexgpt-5-lite-8b-pretrain-q4_k_m.gguf\"\n",
        "MODEL_PATH = \"./yandexgpt-5-lite-8b-pretrain-q4_k_m.gguf\"\n",
        "RAG_DIR = \"/content/rag_db\"\n",
        "CURRENT_CONTEXT_SIZE = 2048  # Default context size\n",
        "CURRENT_GPU_LAYERS = 20      # Default GPU layers"
      ],
      "metadata": {
        "id": "s3ISeNJLidf1"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to download model\n",
        "def download_model(url, save_path):\n",
        "    if os.path.exists(save_path):\n",
        "        return \"Модель уже загружена\"\n",
        "    try:\n",
        "        print(f\"Загрузка модели из {url}...\")\n",
        "        response = requests.get(url, stream=True)\n",
        "        response.raise_for_status()\n",
        "        total_size = int(response.headers.get('content-length', 0))\n",
        "        block_size = 8192\n",
        "        downloaded = 0\n",
        "\n",
        "        with open(save_path, \"wb\") as f:\n",
        "            for chunk in response.iter_content(chunk_size=block_size):\n",
        "                f.write(chunk)\n",
        "                downloaded += len(chunk)\n",
        "                if total_size > 0:\n",
        "                    print(f\"\\rЗагружено: {downloaded/1024/1024:.1f}MB / {total_size/1024/1024:.1f}MB ({downloaded*100/total_size:.1f}%)\", end=\"\")\n",
        "\n",
        "        print(\"\\nМодель успешно загружена\")\n",
        "        return \"Модель успешно загружена\"\n",
        "    except requests.RequestException as e:\n",
        "        return f\"Ошибка загрузки модели: {e}\""
      ],
      "metadata": {
        "id": "yYCvPIOgoPlj"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download model if needed\n",
        "if not os.path.exists(MODEL_PATH):\n",
        "    print(\"Скачивание модели...\")\n",
        "    download_model(MODEL_URL, MODEL_PATH)"
      ],
      "metadata": {
        "id": "fQOVhtF9ijSI"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Добавьте эту функцию перед функциями update_model и change_context_size\n",
        "\n",
        "def load_model_with_params(context_size=CURRENT_CONTEXT_SIZE, gpu_layers=CURRENT_GPU_LAYERS):\n",
        "    \"\"\"\n",
        "    Загружает модель с указанными параметрами размера контекста и количества GPU слоев.\n",
        "\n",
        "    Args:\n",
        "        context_size (int): Размер контекстного окна модели\n",
        "        gpu_layers (int): Количество слоев для выполнения на GPU\n",
        "\n",
        "    Returns:\n",
        "        tuple: (model, mode) - модель и режим работы (GPU/CPU)\n",
        "    \"\"\"\n",
        "    from ctransformers import AutoModelForCausalLM\n",
        "\n",
        "    try:\n",
        "        # Try with GPU support\n",
        "        print(f\"Загрузка модели с контекстом {context_size} и {gpu_layers} GPU слоями...\")\n",
        "        model = AutoModelForCausalLM.from_pretrained(\n",
        "            MODEL_PATH,\n",
        "            model_type=\"llama\",\n",
        "            gpu_layers=gpu_layers,\n",
        "            context_length=context_size,\n",
        "            batch_size=512\n",
        "        )\n",
        "        print(f\"Модель успешно загружена с контекстом {context_size}\")\n",
        "        return model, \"GPU\"\n",
        "    except Exception as e:\n",
        "        print(f\"Ошибка при загрузке модели с GPU: {e}\")\n",
        "        print(\"Загружаем модель в режиме CPU...\")\n",
        "\n",
        "        try:\n",
        "            model = AutoModelForCausalLM.from_pretrained(\n",
        "                MODEL_PATH,\n",
        "                model_type=\"llama\",\n",
        "                gpu_layers=0,  # CPU only mode\n",
        "                context_length=context_size,\n",
        "                batch_size=512\n",
        "            )\n",
        "            print(f\"Модель загружена в режиме CPU с контекстом {context_size}\")\n",
        "            return model, \"CPU\"\n",
        "        except Exception as e:\n",
        "            raise Exception(f\"Не удалось загрузить модель: {e}\")"
      ],
      "metadata": {
        "id": "nzr73Z6EQOic"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create directories\n",
        "os.makedirs(\"/content/Doc\", exist_ok=True)"
      ],
      "metadata": {
        "id": "gcy_vARplMxc"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Global variable to hold the sample document creation function\n",
        "def create_sample_document():\n",
        "    \"\"\"Create a sample document for RAG testing if none exists\"\"\"\n",
        "    sample_path = \"/content/Doc/sample.docx\"\n",
        "    try:\n",
        "        from docx import Document\n",
        "        doc = Document()\n",
        "        doc.add_paragraph(\"Этот документ создан для примера работы системы RAG с УрФУ.\")\n",
        "        doc.add_paragraph(\"Уральский федеральный университет (УрФУ) расположен в Екатеринбурге.\")\n",
        "        doc.add_paragraph(\"УрФУ является одним из ведущих вузов России.\")\n",
        "        doc.add_paragraph(\"В УрФУ обучаются студенты со всей России и из многих зарубежных стран.\")\n",
        "        doc.add_paragraph(\"УрФУ предлагает программы бакалавриата, магистратуры и аспирантуры.\")\n",
        "        doc.save(sample_path)\n",
        "        print(f\"✅ Создан пример документа для тестирования: {sample_path}\")\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Ошибка при создании примера документа: {e}\")\n",
        "        return False\n",
        "\n",
        "# RAG functions with improved error handling\n",
        "def initialize_rag():\n",
        "    \"\"\"Initialize the RAG database with documents\"\"\"\n",
        "    try:\n",
        "        # Ensure the directory exists\n",
        "        os.makedirs(\"/content/Doc\", exist_ok=True)\n",
        "\n",
        "        # Check if there are any documents\n",
        "        doc_files = [f for f in os.listdir(\"/content/Doc\") if f.endswith(\".docx\")]\n",
        "\n",
        "        if not doc_files:\n",
        "            print(\"Нет документов для загрузки. Создаем пример документа...\")\n",
        "            created = create_sample_document()\n",
        "            if not created:\n",
        "                print(\"Не удалось создать пример документа. Инициализация RAG не выполнена.\")\n",
        "                return None\n",
        "\n",
        "        # Load documents\n",
        "        loader = DirectoryLoader(\"/content/Doc\", glob=\"**/*.docx\")\n",
        "        documents = loader.load()\n",
        "\n",
        "        if not documents:\n",
        "            print(\"Не удалось загрузить документы, даже после создания примера.\")\n",
        "            print(\"Проверьте, что папка /content/Doc содержит доступные файлы .docx\")\n",
        "            return None\n",
        "\n",
        "        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "        texts = text_splitter.split_documents(documents)\n",
        "\n",
        "        if not texts:\n",
        "            print(\"Документы загружены, но не удалось извлечь текст.\")\n",
        "            return None\n",
        "\n",
        "        embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\")\n",
        "        vector_db = Chroma.from_documents(texts, embeddings, persist_directory=RAG_DIR)\n",
        "        print(f\"✅ База RAG успешно инициализирована с {len(texts)} фрагментами\")\n",
        "        return vector_db\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Ошибка инициализации RAG: {e}\")\n",
        "        return None\n"
      ],
      "metadata": {
        "id": "gsC97TA2dZ-5"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load or initialize RAG database with better error handling\n",
        "vector_db = None  # Initialize to None first\n",
        "try:\n",
        "    if not os.path.exists(RAG_DIR):\n",
        "        print(\"RAG база данных не найдена, создаем новую...\")\n",
        "        vector_db = initialize_rag()\n",
        "        if vector_db is None:\n",
        "            print(\"❌ Не удалось инициализировать RAG. Ответы модели не будут использовать контекст документов.\")\n",
        "    else:\n",
        "        try:\n",
        "            print(\"Загружаем существующую базу RAG...\")\n",
        "            embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\")\n",
        "            vector_db = Chroma(persist_directory=RAG_DIR, embedding_function=embeddings)\n",
        "            print(\"✅ База RAG успешно загружена\")\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Ошибка загрузки существующей базы RAG: {e}\")\n",
        "            print(\"Пробуем создать новую базу RAG...\")\n",
        "            if os.path.exists(RAG_DIR):\n",
        "                import shutil\n",
        "                shutil.rmtree(RAG_DIR)\n",
        "            vector_db = initialize_rag()\n",
        "            if vector_db is None:\n",
        "                print(\"❌ Не удалось инициализировать RAG. Ответы модели не будут использовать контекст документов.\")\n",
        "except Exception as e:\n",
        "    print(f\"❌ Неожиданная ошибка при работе с RAG: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MU84NQmGoZsl",
        "outputId": "1ea52656-3d5c-4848-fca7-50e344be915b"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RAG база данных не найдена, создаем новую...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ База RAG успешно инициализирована с 24 фрагментами\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Фиксированный контекст, который будет передаваться модели\n",
        "FIXED_CONTEXT = \"\"\"Ты — интеллектуальный помощник, обученный отвечать на вопросы строго в рамках предоставленного контекста. Ты помощник по Уральскому Федеральному университету, который общается с пользователем. Общайся с ним только на русском языке. Объясняй максимально подробно, чтобы пользователь всё понял. Если информации недостаточно, отвечай \"Вы можете ознакомиться с данной информацией на сайте УрФУ\". Избегай выдумок и предположений. Если пользователь просит дать точные значения - обращайся только к данным RAG.\"\"\"\n",
        "CURRENT_TEMPERATURE = 0.5  # Значение по умолчанию\n",
        "# Function to generate response using RAG\n",
        "# Обновленная функция для генерации ответов с улучшенным контролем источников\n",
        "def ask_question_with_rag(question):\n",
        "    try:\n",
        "        # Check if vector_db is available\n",
        "        if vector_db is None:\n",
        "            # Fallback to model-only generation\n",
        "            prompt = f\"{FIXED_CONTEXT}\\n\\nUser: {question}\\nAssistant:\"\n",
        "            response = model(\n",
        "                prompt,\n",
        "                max_new_tokens=256,\n",
        "                temperature=CURRENT_TEMPERATURE,\n",
        "                stop=[\"User:\", \"\\n\\n\"]\n",
        "            )\n",
        "            return response.strip() + \"\\n\\n(Примечание: ответ дан без использования базы знаний, так как RAG не инициализирован)\"\n",
        "\n",
        "        # Retrieve relevant documents\n",
        "        docs = vector_db.similarity_search(question, k=3)\n",
        "\n",
        "        # Check if the retrieved documents are actually relevant by looking for keywords\n",
        "        query_keywords = set(question.lower().split())\n",
        "        important_keywords = {word for word in query_keywords\n",
        "                             if len(word) > 3 and word not in\n",
        "                             ['что', 'как', 'где', 'когда', 'какие', 'какой', 'какая', 'институт', 'урфу', 'университет', 'ИРИТ-РТФ', 'РТФ']}\n",
        "\n",
        "        # Extract and show relevant context\n",
        "        extracted_context = \"\\n\\n\".join([doc.page_content for doc in docs])\n",
        "\n",
        "        # Check if any important keywords are in the context\n",
        "        found_keywords = []\n",
        "        for keyword in important_keywords:\n",
        "            if keyword in extracted_context.lower():\n",
        "                found_keywords.append(keyword)\n",
        "\n",
        "        # Determine relevance score\n",
        "        relevance_score = len(found_keywords) / max(1, len(important_keywords)) if important_keywords else 0.5\n",
        "\n",
        "        # Add strong instruction about only using provided context\n",
        "        strict_instruction = \"\"\"\n",
        "ВАЖНО: Отвечай ТОЛЬКО на основе предоставленной информации из документов.\n",
        "Если в предоставленных документах нет ответа на вопрос, честно скажи\n",
        "\"В документах нет информации о [тема вопроса]. Вы можете ознакомиться с этой информацией на сайте УрФУ.\"\n",
        "НЕ ПРИДУМЫВАЙ информацию, которой нет в документах!\n",
        "\"\"\"\n",
        "        combined_context = f\"{FIXED_CONTEXT}\\n\\n{strict_instruction}\\n\\nДокументы:\\n{extracted_context}\"\n",
        "\n",
        "        # Create prompt for model with stronger guidance\n",
        "        prompt = f\"Context: {combined_context}\\n\\nUser: {question}\\n\\nAssistant:\"\n",
        "\n",
        "        # Use even lower temperature for low relevance scores to reduce hallucination\n",
        "        adjusted_temperature = min(CURRENT_TEMPERATURE, 0.3) if relevance_score < 0.5 else CURRENT_TEMPERATURE\n",
        "\n",
        "        # Generate response with ctransformers\n",
        "        response = model(\n",
        "            prompt,\n",
        "            max_new_tokens=256,\n",
        "            temperature=adjusted_temperature,  # Use adjusted temperature based on relevance\n",
        "            stop=[\"User:\", \"\\n\\n\"]\n",
        "        )\n",
        "\n",
        "        # Return the generated text with optional debugging info\n",
        "        result = response.strip()\n",
        "\n",
        "        # For debugging - uncomment to show relevance information\n",
        "        # debug_info = f\"\\n\\n[Отладка: Найдено {len(found_keywords)}/{len(important_keywords)} ключевых слов, релевантность {relevance_score:.2f}]\"\n",
        "        # return result + debug_info\n",
        "\n",
        "        return result\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"Произошла ошибка при генерации ответа: {str(e)}\\n\\nПожалуйста, попробуйте очистить и переинициализировать RAG.\"\n",
        "\n",
        "# Function to update model\n",
        "def update_model(link):\n",
        "    try:\n",
        "        global model, mode\n",
        "        result = download_model(link, MODEL_PATH)\n",
        "        # Reload with current context size\n",
        "        model, mode = load_model_with_params(CURRENT_CONTEXT_SIZE, CURRENT_GPU_LAYERS)\n",
        "        return f\"Модель обновлена: {result}. Режим работы: {mode}\"\n",
        "    except Exception as e:\n",
        "        return f\"Ошибка при обновлении модели: {str(e)}\"\n",
        "\n",
        "# Function to change context size\n",
        "def change_context_size(new_size_str):\n",
        "    try:\n",
        "        global model, CURRENT_CONTEXT_SIZE, mode\n",
        "\n",
        "        # Convert to integer and validate\n",
        "        new_size = int(new_size_str)\n",
        "        if new_size < 512:\n",
        "            return \"Ошибка: размер контекста должен быть не менее 512\"\n",
        "        if new_size > 8192:\n",
        "            return \"Ошибка: размер контекста не может превышать 8192\"\n",
        "\n",
        "        CURRENT_CONTEXT_SIZE = new_size\n",
        "        model, mode = load_model_with_params(CURRENT_CONTEXT_SIZE, CURRENT_GPU_LAYERS)\n",
        "        return f\"Размер контекста изменен на {new_size}. Режим работы: {mode}\"\n",
        "    except ValueError:\n",
        "        return \"Ошибка: введите корректное целое число\"\n",
        "    except Exception as e:\n",
        "        return f\"Ошибка при изменении размера контекста: {str(e)}\"\n",
        "\n",
        "def change_temperature(new_temp_str):\n",
        "    try:\n",
        "        global CURRENT_TEMPERATURE\n",
        "\n",
        "        # Convert to float and validate\n",
        "        new_temp = float(new_temp_str)\n",
        "        if new_temp < 0.0:\n",
        "            return \"Ошибка: температура не может быть меньше 0.0\"\n",
        "        if new_temp > 2.0:\n",
        "            return \"Ошибка: температура не рекомендуется выше 2.0\"\n",
        "\n",
        "        CURRENT_TEMPERATURE = new_temp\n",
        "        return f\"Температура генерации изменена на {new_temp}\"\n",
        "    except ValueError:\n",
        "        return \"Ошибка: введите корректное число с плавающей точкой (например, 0.2)\"\n",
        "    except Exception as e:\n",
        "        return f\"Ошибка при изменении температуры: {str(e)}\"\n",
        "\n",
        "# Function to clear RAG\n",
        "def clear_rag():\n",
        "    try:\n",
        "        if os.path.exists(RAG_DIR):\n",
        "            import shutil\n",
        "            shutil.rmtree(RAG_DIR)\n",
        "        global vector_db\n",
        "        vector_db = initialize_rag()\n",
        "        return \"RAG очищен и переинициализирован\"\n",
        "    except Exception as e:\n",
        "        return f\"Ошибка при очистке RAG: {str(e)}\""
      ],
      "metadata": {
        "id": "JxfLy4ntbGML"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model, mode = load_model_with_params(CURRENT_CONTEXT_SIZE, CURRENT_GPU_LAYERS)\n",
        "\n",
        "\n",
        "# Исправленная версия Gradio UI с правильными отступами\n",
        "with gr.Blocks() as demo:\n",
        "    with gr.Row():\n",
        "        gr.Markdown(\"### Чат с AI\")\n",
        "    chat_input = gr.Textbox(label=\"Введите вопрос\")\n",
        "    chat_output = gr.Textbox(label=\"Ответ\", lines=10)\n",
        "    submit_button = gr.Button(\"Отправить\")\n",
        "    submit_button.click(ask_question_with_rag, inputs=chat_input, outputs=chat_output)\n",
        "\n",
        "    with gr.Row():\n",
        "        gr.Markdown(\"### Админ-панель\")\n",
        "\n",
        "    with gr.Row():\n",
        "        with gr.Column():\n",
        "            model_link = gr.Textbox(label=\"Ссылка на модель\")\n",
        "            update_model_button = gr.Button(\"Обновить модель\")\n",
        "            update_model_output = gr.Textbox(label=\"Статус обновления модели\")\n",
        "            update_model_button.click(update_model, inputs=model_link, outputs=update_model_output)\n",
        "\n",
        "        with gr.Column():\n",
        "            context_size_input = gr.Textbox(label=\"Размер контекста (512-8192)\", value=str(CURRENT_CONTEXT_SIZE))\n",
        "            context_size_button = gr.Button(\"Изменить размер контекста\")\n",
        "            context_size_output = gr.Textbox(label=\"Статус изменения контекста\")\n",
        "            context_size_button.click(change_context_size, inputs=context_size_input, outputs=context_size_output)\n",
        "\n",
        "    with gr.Row():\n",
        "        with gr.Column():\n",
        "            temperature_input = gr.Textbox(\n",
        "                label=\"Температура (0.0-2.0, меньше = точнее, больше = креативнее)\",\n",
        "                value=str(CURRENT_TEMPERATURE)\n",
        "            )\n",
        "            temperature_button = gr.Button(\"Изменить температуру\")\n",
        "            temperature_output = gr.Textbox(label=\"Статус изменения температуры\")\n",
        "            temperature_button.click(change_temperature, inputs=temperature_input, outputs=temperature_output)\n",
        "\n",
        "        with gr.Column():\n",
        "            rag_button = gr.Button(\"Очистить RAG\")\n",
        "            rag_output = gr.Textbox(label=\"Статус очистки RAG\")\n",
        "            rag_button.click(clear_rag, outputs=rag_output)\n",
        "\n",
        "# Launch with a specific share setting\n",
        "print(\"\\nЗапуск интерфейса Gradio...\")\n",
        "demo.launch(debug=True, share=True)"
      ],
      "metadata": {
        "id": "NIBY23V2CIZk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 730
        },
        "outputId": "f88d2c2e-ed18-4b7a-ba35-6e1ab195e26e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Загрузка модели с контекстом 2048 и 20 GPU слоями...\n",
            "Модель успешно загружена с контекстом 2048\n",
            "\n",
            "Запуск интерфейса Gradio...\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "IMPORTANT: You are using gradio version 3.50.2, however version 4.44.1 is available, please upgrade.\n",
            "--------\n",
            "Running on public URL: https://406d4cea5b87b8fd3e.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://406d4cea5b87b8fd3e.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Загрузка модели с контекстом 8192 и 20 GPU слоями...\n",
            "Модель успешно загружена с контекстом 8192\n"
          ]
        }
      ]
    }
  ]
}