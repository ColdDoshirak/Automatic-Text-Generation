{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "После ошибки при Import Gradio нужно в Среде выполнения перезапустить сеанс и снова выполнить весь код"
      ],
      "metadata": {
        "id": "eoaubCx2nR8B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Также нужно создать папку Doc и поместить туда docx (если его нет - код создаст заглушку)"
      ],
      "metadata": {
        "id": "_upy9HtCULk3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LCL5pcWOr_Jj"
      },
      "outputs": [],
      "source": [
        "!apt-get -y install poppler-utils tesseract-ocr libtesseract-dev tesseract-ocr-rus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l3uZuhM0m3w3"
      },
      "outputs": [],
      "source": [
        "!pip uninstall -y gradio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V1W40VHi8qBo"
      },
      "outputs": [],
      "source": [
        "# Cell 3 (Modified Again)\n",
        "!pip install -q numpy ctransformers[cuda] sentence-transformers chromadb langchain langchain-community langchain-huggingface gradio unstructured unstructured[pdf] unstructured[docx] python-docx fastapi uvicorn[standard] nest_asyncio pyngrok openai-whisper --quiet sse-starlette"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8io1m_xTodiq"
      },
      "outputs": [],
      "source": [
        "# Install ctransformers with CPU support first (more reliable than GPU on some configurations)\n",
        "!pip install -q ctransformers --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1b4KZvSE-bVW"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import gradio as gr\n",
        "import requests\n",
        "from ctransformers import AutoModelForCausalLM\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.document_loaders import DirectoryLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "\n",
        "# --- New API Imports ---\n",
        "import uvicorn\n",
        "from fastapi import FastAPI, HTTPException, Request\n",
        "from fastapi.responses import JSONResponse\n",
        "from pydantic import BaseModel\n",
        "import whisper\n",
        "import tempfile\n",
        "# --- End New API Imports ---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RWE_gUQYlClK"
      },
      "outputs": [],
      "source": [
        "# Check GPU availability\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s3ISeNJLidf1"
      },
      "outputs": [],
      "source": [
        "# Model settings\n",
        "MODEL_URL = \"https://huggingface.co/mradermacher/YandexGPT-5-Lite-8B-instruct-GGUF/resolve/main/YandexGPT-5-Lite-8B-instruct.Q4_K_M.gguf\"\n",
        "MODEL_PATH = \"./YandexGPT-5-Lite-8B-instruct.Q4_K_M.gguf\"\n",
        "RAG_DIR = \"/content/rag_db\"\n",
        "CURRENT_CONTEXT_SIZE = 4096  # Default context size\n",
        "CURRENT_GPU_LAYERS = 24     # Default GPU layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yYCvPIOgoPlj"
      },
      "outputs": [],
      "source": [
        "# Function to download model\n",
        "def download_model(url, save_path):\n",
        "    if os.path.exists(save_path):\n",
        "        return \"Модель уже загружена\"\n",
        "    try:\n",
        "        print(f\"Загрузка модели из {url}...\")\n",
        "        response = requests.get(url, stream=True)\n",
        "        response.raise_for_status()\n",
        "        total_size = int(response.headers.get('content-length', 0))\n",
        "        block_size = 8192\n",
        "        downloaded = 0\n",
        "\n",
        "        with open(save_path, \"wb\") as f:\n",
        "            for chunk in response.iter_content(chunk_size=block_size):\n",
        "                f.write(chunk)\n",
        "                downloaded += len(chunk)\n",
        "                if total_size > 0:\n",
        "                    print(f\"\\rЗагружено: {downloaded/1024/1024:.1f}MB / {total_size/1024/1024:.1f}MB ({downloaded*100/total_size:.1f}%)\", end=\"\")\n",
        "\n",
        "        print(\"\\nМодель успешно загружена\")\n",
        "        return \"Модель успешно загружена\"\n",
        "    except requests.RequestException as e:\n",
        "        return f\"Ошибка загрузки модели: {e}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fQOVhtF9ijSI"
      },
      "outputs": [],
      "source": [
        "# Download model if needed\n",
        "if not os.path.exists(MODEL_PATH):\n",
        "    print(\"Скачивание модели...\")\n",
        "    download_model(MODEL_URL, MODEL_PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nzr73Z6EQOic"
      },
      "outputs": [],
      "source": [
        "# Добавьте эту функцию перед функциями update_model и change_context_size\n",
        "\n",
        "def load_model_with_params(context_size=CURRENT_CONTEXT_SIZE, gpu_layers=CURRENT_GPU_LAYERS):\n",
        "    \"\"\"\n",
        "    Загружает модель с указанными параметрами размера контекста и количества GPU слоев.\n",
        "\n",
        "    Args:\n",
        "        context_size (int): Размер контекстного окна модели\n",
        "        gpu_layers (int): Количество слоев для выполнения на GPU\n",
        "\n",
        "    Returns:\n",
        "        tuple: (model, mode) - модель и режим работы (GPU/CPU)\n",
        "    \"\"\"\n",
        "    from ctransformers import AutoModelForCausalLM\n",
        "\n",
        "    try:\n",
        "        # Try with GPU support\n",
        "        print(f\"Загрузка модели с контекстом {context_size} и {gpu_layers} GPU слоями...\")\n",
        "        model = AutoModelForCausalLM.from_pretrained(\n",
        "            MODEL_PATH,\n",
        "            model_type=\"llama\",\n",
        "            gpu_layers=gpu_layers,\n",
        "            context_length=context_size,\n",
        "            batch_size=512\n",
        "        )\n",
        "        print(f\"Модель успешно загружена с контекстом {context_size}\")\n",
        "        return model, \"GPU\"\n",
        "    except Exception as e:\n",
        "        print(f\"Ошибка при загрузке модели с GPU: {e}\")\n",
        "        print(\"Загружаем модель в режиме CPU...\")\n",
        "\n",
        "        try:\n",
        "            model = AutoModelForCausalLM.from_pretrained(\n",
        "                MODEL_PATH,\n",
        "                model_type=\"llama\",\n",
        "                gpu_layers=0,  # CPU only mode\n",
        "                context_length=context_size,\n",
        "                batch_size=512\n",
        "            )\n",
        "            print(f\"Модель загружена в режиме CPU с контекстом {context_size}\")\n",
        "            return model, \"CPU\"\n",
        "        except Exception as e:\n",
        "            raise Exception(f\"Не удалось загрузить модель: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Whisper implementation\n",
        "whisper_model = None\n",
        "\n",
        "def load_whisper_model(model_size=\"small\"):\n",
        "    \"\"\"\n",
        "    Load Whisper model for speech recognition.\n",
        "\n",
        "    Args:\n",
        "        model_size (str): Size of the Whisper model to load.\n",
        "                    Options: \"tiny\", \"base\", \"small\", \"medium\", \"large\"\n",
        "\n",
        "    Returns:\n",
        "        The loaded Whisper model\n",
        "    \"\"\"\n",
        "    global whisper_model\n",
        "    try:\n",
        "        print(f\"Loading Whisper {model_size} model...\")\n",
        "        whisper_model = whisper.load_model(model_size)\n",
        "        print(f\"Whisper {model_size} model loaded successfully\")\n",
        "        return whisper_model\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading Whisper model: {e}\")\n",
        "        return None\n",
        "\n",
        "def transcribe_audio(audio_file):\n",
        "    \"\"\"\n",
        "    Transcribe audio file using Whisper model.\n",
        "\n",
        "    Args:\n",
        "        audio_file: Path to the audio file or audio file object\n",
        "\n",
        "    Returns:\n",
        "        str: Transcribed text\n",
        "    \"\"\"\n",
        "    global whisper_model\n",
        "\n",
        "    try:\n",
        "        # Load model if not loaded\n",
        "        if whisper_model is None:\n",
        "            whisper_model = load_whisper_model()\n",
        "            if whisper_model is None:\n",
        "                return \"Ошибка: Не удалось загрузить модель Whisper\"\n",
        "\n",
        "        # Handle file path or file object\n",
        "        temp_file = None\n",
        "        if isinstance(audio_file, str):\n",
        "            file_path = audio_file\n",
        "        else:\n",
        "            # Save to a temporary file if it's a file object\n",
        "            temp_file = tempfile.NamedTemporaryFile(delete=False, suffix=\".wav\")\n",
        "            temp_file.close()\n",
        "            audio_file.save(temp_file.name)\n",
        "            file_path = temp_file.name\n",
        "\n",
        "        # Transcribe audio\n",
        "        result = whisper_model.transcribe(file_path)\n",
        "        transcription = result[\"text\"].strip()\n",
        "\n",
        "        # Clean up temp file if created\n",
        "        if temp_file is not None:\n",
        "            os.unlink(temp_file.name)\n",
        "\n",
        "        return transcription\n",
        "    except Exception as e:\n",
        "        return f\"Ошибка транскрибирования аудио: {str(e)}\""
      ],
      "metadata": {
        "id": "iBTGEKPdZILM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gcy_vARplMxc"
      },
      "outputs": [],
      "source": [
        "# Create directories\n",
        "os.makedirs(\"/content/Doc\", exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gsC97TA2dZ-5"
      },
      "outputs": [],
      "source": [
        "# Global variable to hold the sample document creation function\n",
        "def create_sample_document():\n",
        "    \"\"\"Create a sample document for RAG testing if none exists\"\"\"\n",
        "    sample_path = \"/content/Doc/sample.docx\"\n",
        "    try:\n",
        "        from docx import Document\n",
        "        doc = Document()\n",
        "        doc.add_paragraph(\"Этот документ создан для примера работы системы RAG с УрФУ.\")\n",
        "        doc.add_paragraph(\"Уральский федеральный университет (УрФУ) расположен в Екатеринбурге.\")\n",
        "        doc.add_paragraph(\"УрФУ является одним из ведущих вузов России.\")\n",
        "        doc.add_paragraph(\"В УрФУ обучаются студенты со всей России и из многих зарубежных стран.\")\n",
        "        doc.add_paragraph(\"УрФУ предлагает программы бакалавриата, магистратуры и аспирантуры.\")\n",
        "        doc.save(sample_path)\n",
        "        print(f\"✅ Создан пример документа для тестирования: {sample_path}\")\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Ошибка при создании примера документа: {e}\")\n",
        "        return False\n",
        "\n",
        "# RAG functions with improved error handling\n",
        "def initialize_rag():\n",
        "    \"\"\"Initialize the RAG database with documents\"\"\"\n",
        "    try:\n",
        "        # Ensure the directory exists\n",
        "        os.makedirs(\"/content/Doc\", exist_ok=True)\n",
        "\n",
        "        # Check if there are any documents\n",
        "        doc_files = [f for f in os.listdir(\"/content/Doc\") if f.endswith(\".docx\")]\n",
        "\n",
        "        if not doc_files:\n",
        "            print(\"Нет документов для загрузки. Создаем пример документа...\")\n",
        "            created = create_sample_document()\n",
        "            if not created:\n",
        "                print(\"Не удалось создать пример документа. Инициализация RAG не выполнена.\")\n",
        "                return None\n",
        "\n",
        "        # Load documents\n",
        "        loader = DirectoryLoader(\"/content/Doc\", glob=\"**/*.docx\")\n",
        "        documents = loader.load()\n",
        "\n",
        "        if not documents:\n",
        "            print(\"Не удалось загрузить документы, даже после создания примера.\")\n",
        "            print(\"Проверьте, что папка /content/Doc содержит доступные файлы .docx\")\n",
        "            return None\n",
        "\n",
        "        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "        texts = text_splitter.split_documents(documents)\n",
        "\n",
        "        if not texts:\n",
        "            print(\"Документы загружены, но не удалось извлечь текст.\")\n",
        "            return None\n",
        "\n",
        "        embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\")\n",
        "        vector_db = Chroma.from_documents(texts, embeddings, persist_directory=RAG_DIR)\n",
        "        print(f\"✅ База RAG успешно инициализирована с {len(texts)} фрагментами\")\n",
        "        return vector_db\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Ошибка инициализации RAG: {e}\")\n",
        "        return None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MU84NQmGoZsl"
      },
      "outputs": [],
      "source": [
        "# Load or initialize RAG database with better error handling\n",
        "vector_db = None  # Initialize to None first\n",
        "try:\n",
        "    if not os.path.exists(RAG_DIR):\n",
        "        print(\"RAG база данных не найдена, создаем новую...\")\n",
        "        vector_db = initialize_rag()\n",
        "        if vector_db is None:\n",
        "            print(\"❌ Не удалось инициализировать RAG. Ответы модели не будут использовать контекст документов.\")\n",
        "    else:\n",
        "        try:\n",
        "            print(\"Загружаем существующую базу RAG...\")\n",
        "            embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\")\n",
        "            vector_db = Chroma(persist_directory=RAG_DIR, embedding_function=embeddings)\n",
        "            print(\"✅ База RAG успешно загружена\")\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Ошибка загрузки существующей базы RAG: {e}\")\n",
        "            print(\"Пробуем создать новую базу RAG...\")\n",
        "            if os.path.exists(RAG_DIR):\n",
        "                import shutil\n",
        "                shutil.rmtree(RAG_DIR)\n",
        "            vector_db = initialize_rag()\n",
        "            if vector_db is None:\n",
        "                print(\"❌ Не удалось инициализировать RAG. Ответы модели не будут использовать контекст документов.\")\n",
        "except Exception as e:\n",
        "    print(f\"❌ Неожиданная ошибка при работе с RAG: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JxfLy4ntbGML"
      },
      "outputs": [],
      "source": [
        "# Фиксированный контекст, который будет передаваться модели\n",
        "FIXED_CONTEXT = \"\"\"Ты — интеллектуальный помощник, обученный отвечать на вопросы строго в рамках предоставленного контекста. Ты помощник по Уральскому Федеральному университету, который общается с пользователем. Общайся с ним на удобном ему языке. Объясняй максимально подробно, чтобы пользователь всё понял. Если информации недостаточно, отвечай \"Вы можете ознакомиться с данной информацией на сайте УрФУ\". Избегай выдумок и предположений. Если пользователь просит дать точные значения - обращайся только к данным RAG.\"\"\"\n",
        "CURRENT_TEMPERATURE = 0.5  # Значение по умолчанию\n",
        "# Function to generate response using RAG\n",
        "# Обновленная функция для генерации ответов с улучшенным контролем источников\n",
        "def ask_question_with_rag(question):\n",
        "    try:\n",
        "        # Check if vector_db is available\n",
        "        if vector_db is None:\n",
        "            # Fallback to model-only generation\n",
        "            prompt = f\"{FIXED_CONTEXT}\\n\\nUser: {question}\\nAssistant:\"\n",
        "            response = model(\n",
        "                prompt,\n",
        "                max_new_tokens=256,\n",
        "                temperature=CURRENT_TEMPERATURE,\n",
        "                stop=[\"User:\", \"\\n\\n\"]\n",
        "            )\n",
        "            return response.strip() + \"\\n\\n(Примечание: ответ дан без использования базы знаний, так как RAG не инициализирован)\"\n",
        "\n",
        "        # Retrieve relevant documents\n",
        "        docs = vector_db.similarity_search(question, k=3)\n",
        "\n",
        "        # Check if the retrieved documents are actually relevant by looking for keywords\n",
        "        query_keywords = set(question.lower().split())\n",
        "        important_keywords = {word for word in query_keywords\n",
        "                             if len(word) > 3 and word not in\n",
        "                             ['что', 'как', 'где', 'когда', 'какие', 'какой', 'какая', 'институт', 'урфу', 'университет', 'ИРИТ-РТФ', 'РТФ']}\n",
        "\n",
        "        # Extract and show relevant context\n",
        "        extracted_context = \"\\n\\n\".join([doc.page_content for doc in docs])\n",
        "\n",
        "        # Check if any important keywords are in the context\n",
        "        found_keywords = []\n",
        "        for keyword in important_keywords:\n",
        "            if keyword in extracted_context.lower():\n",
        "                found_keywords.append(keyword)\n",
        "\n",
        "        # Determine relevance score\n",
        "        relevance_score = len(found_keywords) / max(1, len(important_keywords)) if important_keywords else 0.5\n",
        "\n",
        "        # Add strong instruction about only using provided context\n",
        "        #strict_instruction = \"\"\"\n",
        "#ВАЖНО: Отвечай ТОЛЬКО на основе предоставленной информации из документов.\n",
        "#Если в предоставленных документах нет ответа на вопрос, честно скажи\n",
        "#\"В документах нет информации о [тема вопроса]. Вы можете ознакомиться с этой информацией на сайте УрФУ.\"\n",
        "#НЕ ПРИДУМЫВАЙ информацию, которой нет в документах!\n",
        "#\"\"\"\n",
        "\n",
        "        strict_instruction = \"\"\n",
        "\n",
        "        combined_context = f\"{FIXED_CONTEXT}\\n\\n{strict_instruction}\\n\\nДокументы:\\n{extracted_context}\"\n",
        "\n",
        "        # Create prompt for model with stronger guidance\n",
        "        prompt = f\"Context: {combined_context}\\n\\nUser: {question}\\n\\nAssistant:\"\n",
        "\n",
        "        # Use even lower temperature for low relevance scores to reduce hallucination\n",
        "        adjusted_temperature = min(CURRENT_TEMPERATURE, 0.3) if relevance_score < 0.5 else CURRENT_TEMPERATURE\n",
        "\n",
        "        # Generate response with ctransformers\n",
        "        response = model(\n",
        "            prompt,\n",
        "            max_new_tokens=256,\n",
        "            temperature=adjusted_temperature,  # Use adjusted temperature based on relevance\n",
        "            stop=[\"User:\", \"\\n\\n\"]\n",
        "        )\n",
        "\n",
        "        # Return the generated text with optional debugging info\n",
        "        result = response.strip()\n",
        "\n",
        "        # For debugging - uncomment to show relevance information\n",
        "        # debug_info = f\"\\n\\n[Отладка: Найдено {len(found_keywords)}/{len(important_keywords)} ключевых слов, релевантность {relevance_score:.2f}]\"\n",
        "        # return result + debug_info\n",
        "\n",
        "        return result\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"Произошла ошибка при генерации ответа: {str(e)}\\n\\nПожалуйста, попробуйте очистить и переинициализировать RAG.\"\n",
        "\n",
        "# Function to update model\n",
        "def update_model(link):\n",
        "    try:\n",
        "        global model, mode\n",
        "        result = download_model(link, MODEL_PATH)\n",
        "        # Reload with current context size\n",
        "        model, mode = load_model_with_params(CURRENT_CONTEXT_SIZE, CURRENT_GPU_LAYERS)\n",
        "        return f\"Модель обновлена: {result}. Режим работы: {mode}\"\n",
        "    except Exception as e:\n",
        "        return f\"Ошибка при обновлении модели: {str(e)}\"\n",
        "\n",
        "# Function to change context size\n",
        "def change_context_size(new_size_str):\n",
        "    try:\n",
        "        global model, CURRENT_CONTEXT_SIZE, mode\n",
        "\n",
        "        # Convert to integer and validate\n",
        "        new_size = int(new_size_str)\n",
        "        if new_size < 512:\n",
        "            return \"Ошибка: размер контекста должен быть не менее 512\"\n",
        "        if new_size > 8192:\n",
        "            return \"Ошибка: размер контекста не может превышать 8192\"\n",
        "\n",
        "        CURRENT_CONTEXT_SIZE = new_size\n",
        "        model, mode = load_model_with_params(CURRENT_CONTEXT_SIZE, CURRENT_GPU_LAYERS)\n",
        "        return f\"Размер контекста изменен на {new_size}. Режим работы: {mode}\"\n",
        "    except ValueError:\n",
        "        return \"Ошибка: введите корректное целое число\"\n",
        "    except Exception as e:\n",
        "        return f\"Ошибка при изменении размера контекста: {str(e)}\"\n",
        "\n",
        "def change_temperature(new_temp_str):\n",
        "    try:\n",
        "        global CURRENT_TEMPERATURE\n",
        "\n",
        "        # Convert to float and validate\n",
        "        new_temp = float(new_temp_str)\n",
        "        if new_temp < 0.0:\n",
        "            return \"Ошибка: температура не может быть меньше 0.0\"\n",
        "        if new_temp > 2.0:\n",
        "            return \"Ошибка: температура не рекомендуется выше 2.0\"\n",
        "\n",
        "        CURRENT_TEMPERATURE = new_temp\n",
        "        return f\"Температура генерации изменена на {new_temp}\"\n",
        "    except ValueError:\n",
        "        return \"Ошибка: введите корректное число с плавающей точкой (например, 0.2)\"\n",
        "    except Exception as e:\n",
        "        return f\"Ошибка при изменении температуры: {str(e)}\"\n",
        "\n",
        "# Function to clear RAG\n",
        "def clear_rag():\n",
        "    try:\n",
        "        if os.path.exists(RAG_DIR):\n",
        "            import shutil\n",
        "            shutil.rmtree(RAG_DIR)\n",
        "        global vector_db\n",
        "        vector_db = initialize_rag()\n",
        "        return \"RAG очищен и переинициализирован\"\n",
        "    except Exception as e:\n",
        "        return f\"Ошибка при очистке RAG: {str(e)}\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Streaming version of question answering\n",
        "def ask_question_with_rag_stream(question):\n",
        "    \"\"\"\n",
        "    Streaming version of ask_question_with_rag\n",
        "\n",
        "    Args:\n",
        "        question (str): The question to answer\n",
        "\n",
        "    Yields:\n",
        "        str: Generated text tokens\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Check if vector_db is available\n",
        "        if vector_db is None:\n",
        "            # Fallback to model-only generation\n",
        "            prompt = f\"{FIXED_CONTEXT}\\n\\nUser: {question}\\nAssistant:\"\n",
        "\n",
        "            # Generate tokens\n",
        "            for token in model(\n",
        "                prompt,\n",
        "                max_new_tokens=256,\n",
        "                temperature=CURRENT_TEMPERATURE,\n",
        "                stop=[\"User:\", \"\\n\\n\"],\n",
        "                stream=True\n",
        "            ):\n",
        "                yield token\n",
        "\n",
        "            yield \"\\n\\n(Примечание: ответ дан без использования базы знаний, так как RAG не инициализирован)\"\n",
        "            return\n",
        "\n",
        "        # Retrieve relevant documents\n",
        "        docs = vector_db.similarity_search(question, k=3)\n",
        "\n",
        "        # Check if retrieved documents are actually relevant by looking for keywords\n",
        "        query_keywords = set(question.lower().split())\n",
        "        important_keywords = {word for word in query_keywords\n",
        "                         if len(word) > 3 and word not in\n",
        "                         ['что', 'как', 'где', 'когда', 'какие', 'какой', 'какая', 'институт', 'урфу', 'университет', 'ИРИТ-РТФ', 'РТФ']}\n",
        "\n",
        "        # Extract context\n",
        "        extracted_context = \"\\n\\n\".join([doc.page_content for doc in docs])\n",
        "\n",
        "        # Check if any important keywords are in the context\n",
        "        found_keywords = []\n",
        "        for keyword in important_keywords:\n",
        "            if keyword in extracted_context.lower():\n",
        "                found_keywords.append(keyword)\n",
        "\n",
        "        # Determine relevance score\n",
        "        relevance_score = len(found_keywords) / max(1, len(important_keywords)) if important_keywords else 0.5\n",
        "\n",
        "        # Add strong instruction about only using provided context\n",
        "        #strict_instruction = \"\"\"\n",
        "#ВАЖНО: Отвечай ТОЛЬКО на основе предоставленной информации из документов.\n",
        "#Если в предоставленных документах нет ответа на вопрос, честно скажи\n",
        "#\"В документах нет информации о [тема вопроса]. Вы можете ознакомиться с этой информацией на сайте УрФУ.\"\n",
        "#НЕ ПРИДУМЫВАЙ информацию, которой нет в документах!\n",
        "#\"\"\"\n",
        "\n",
        "        strict_instruction = \"\"\n",
        "\n",
        "        combined_context = f\"{FIXED_CONTEXT}\\n\\n{strict_instruction}\\n\\nДокументы:\\n{extracted_context}\"\n",
        "\n",
        "        # Create prompt for model with stronger guidance\n",
        "        prompt = f\"Context: {combined_context}\\n\\nUser: {question}\\n\\nAssistant:\"\n",
        "\n",
        "        # Use lower temperature for low relevance scores to reduce hallucination\n",
        "        adjusted_temperature = min(CURRENT_TEMPERATURE, 0.3) if relevance_score < 0.5 else CURRENT_TEMPERATURE\n",
        "\n",
        "        # Generate response with ctransformers in streaming mode\n",
        "        for token in model(\n",
        "            prompt,\n",
        "            max_new_tokens=256,\n",
        "            temperature=adjusted_temperature,\n",
        "            stop=[\"User:\", \"\\n\\n\"],\n",
        "            stream=True\n",
        "        ):\n",
        "            yield token\n",
        "\n",
        "    except Exception as e:\n",
        "        yield f\"Произошла ошибка при генерации ответа: {str(e)}\\n\\nПожалуйста, попробуйте очистить и переинициализировать RAG.\""
      ],
      "metadata": {
        "id": "MD6zCOibZdNs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fTOCtgJu4vCO"
      },
      "outputs": [],
      "source": [
        "# Cell 14.1 (New) - API Models\n",
        "class QuestionRequest(BaseModel):\n",
        "    question: str\n",
        "\n",
        "class AnswerResponse(BaseModel):\n",
        "    answer: str"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TjtfQVuu8J3t"
      },
      "outputs": [],
      "source": [
        "# Cell 14.5 (New) - Install cloudflared\n",
        "print(\"Установка cloudflared...\")\n",
        "!wget -q https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64 -O cloudflared\n",
        "!chmod +x cloudflared\n",
        "print(\"cloudflared установлен.\")\n",
        "# Переместим в /usr/local/bin для удобства вызова (не обязательно, но рекомендуется)\n",
        "!mv cloudflared /usr/local/bin/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 426
        },
        "id": "NIBY23V2CIZk",
        "outputId": "f7a8ea40-4445-4519-c9df-d04493c41885"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:     Shutting down\n",
            "INFO:     Waiting for application shutdown.\n",
            "INFO:     Application shutdown complete.\n",
            "INFO:     Finished server process [370]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Завершение cloudflared...\n",
            "Процесс cloudflared завершен.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-31-d4dad17eb212>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[0;31m# --- Запуск ---\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Запуск асинхронной функции для сервера и туннеля...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 403\u001b[0;31m \u001b[0masyncio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_server_and_tunnel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/lib/python3.11/asyncio/tasks.py\u001b[0m in \u001b[0;36m__wakeup\u001b[0;34m(self, future)\u001b[0m\n\u001b[1;32m    347\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__wakeup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 349\u001b[0;31m             \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    350\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m             \u001b[0;31m# This may also be a cancellation.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/asyncio/futures.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    201\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__log_traceback\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 203\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception_tb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    204\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/asyncio/tasks.py\u001b[0m in \u001b[0;36m__step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    275\u001b[0m                 \u001b[0;31m# We use the `send` method directly, because coroutines\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m                 \u001b[0;31m# don't have `__iter__` and `__next__` methods.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 277\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcoro\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcoro\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthrow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/uvicorn/server.py\u001b[0m in \u001b[0;36mserve\u001b[0;34m(self, sockets)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;32masync\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mserve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msockets\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msocket\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcapture_signals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mawait\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_serve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msockets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, typ, value, traceback)\u001b[0m\n\u001b[1;32m    142\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtyp\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m                 \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/uvicorn/server.py\u001b[0m in \u001b[0;36mcapture_signals\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    328\u001b[0m         \u001b[0;31m# done LIFO, see https://stackoverflow.com/questions/48434964\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcaptured_signal\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreversed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_captured_signals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 330\u001b[0;31m             \u001b[0msignal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_signal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcaptured_signal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    331\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mhandle_exit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msig\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mFrameType\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Cell 15 (Updated with Whisper and Streaming)\n",
        "\n",
        "import asyncio\n",
        "import nest_asyncio\n",
        "import uvicorn\n",
        "import os\n",
        "import subprocess\n",
        "import re\n",
        "import time\n",
        "from pydantic import BaseModel\n",
        "from fastapi import FastAPI, HTTPException, UploadFile, File, BackgroundTasks\n",
        "from fastapi.responses import StreamingResponse\n",
        "import gradio as gr\n",
        "from sse_starlette.sse import EventSourceResponse\n",
        "import logging\n",
        "\n",
        "# --- API Models ---\n",
        "class QuestionRequest(BaseModel):\n",
        "    question: str\n",
        "\n",
        "class AnswerResponse(BaseModel):\n",
        "    answer: str\n",
        "\n",
        "# --- 1. Load Model and Initialize RAG (as before) ---\n",
        "print(\"Загрузка модели...\")\n",
        "model, mode = load_model_with_params(CURRENT_CONTEXT_SIZE, CURRENT_GPU_LAYERS)\n",
        "print(f\"Модель загружена. Режим: {mode}\")\n",
        "# Убедитесь, что RAG инициализирован (vector_db существует)\n",
        "\n",
        "# Load Whisper model\n",
        "whisper_model = load_whisper_model()\n",
        "\n",
        "# --- 2. Define Gradio UI (Chat + Admin Panel + Whisper) ---\n",
        "print(\"Определение интерфейса Gradio (Чат + Админ-панель + Whisper)...\")\n",
        "\n",
        "# Whisper processing functions\n",
        "def process_audio_and_fill_chat(audio):\n",
        "    \"\"\"Process audio with Whisper and just fill the chat input\"\"\"\n",
        "    transcription = transcribe_audio(audio)\n",
        "    return transcription, \"\"  # Return to fill input field\n",
        "\n",
        "def process_audio_and_answer(audio):\n",
        "    \"\"\"Process audio with Whisper and directly generate an answer\"\"\"\n",
        "    transcription = transcribe_audio(audio)\n",
        "    # Generate complete answer (non-streaming)\n",
        "    answer = ask_question_with_rag(transcription)\n",
        "    return transcription, answer\n",
        "\n",
        "# Streaming chat handler\n",
        "def submit_message_streaming(message, history):\n",
        "    history.append((message, \"\"))\n",
        "    response_parts = []\n",
        "\n",
        "    for text in ask_question_with_rag_stream(message):\n",
        "        response_parts.append(text)\n",
        "        history[-1] = (message, \"\".join(response_parts))\n",
        "        yield history\n",
        "\n",
        "with gr.Blocks() as demo:\n",
        "    # --- Чат с поддержкой стриминга ---\n",
        "    demo.queue()\n",
        "\n",
        "    with gr.Row():\n",
        "        gr.Markdown(\"### Чат с AI\")\n",
        "\n",
        "    chatbot = gr.Chatbot(label=\"История чата\", height=400)\n",
        "    chat_input = gr.Textbox(label=\"Введите вопрос\", lines=2)\n",
        "\n",
        "    with gr.Row():\n",
        "        submit_button = gr.Button(\"Отправить\")\n",
        "        clear_button = gr.Button(\"Очистить чат\")\n",
        "\n",
        "    # --- Whisper Audio Input ---\n",
        "    with gr.Row():\n",
        "        gr.Markdown(\"### Голосовой ввод\")\n",
        "\n",
        "    with gr.Row():\n",
        "        # Изменяем аудио компонент для поддержки микрофона И загрузки файла\n",
        "        audio_input = gr.Audio(\n",
        "            sources=[\"microphone\", \"upload\"],  # Добавляем возможность загрузки файла\n",
        "            type=\"filepath\",\n",
        "            label=\"Запишите или загрузите голосовой вопрос\",\n",
        "            interactive=True\n",
        "        )\n",
        "        whisper_mode = gr.Radio(\n",
        "            [\"Заполнить поле ввода\", \"Сразу получить ответ\"],\n",
        "            label=\"Режим обработки голоса\",\n",
        "            value=\"Заполнить поле ввода\"\n",
        "        )\n",
        "\n",
        "    with gr.Row():\n",
        "        transcribe_button = gr.Button(\"Обработать аудио\", variant=\"primary\")  # Добавляем variant для визуального выделения\n",
        "\n",
        "    # --- Admin Panel ---\n",
        "    with gr.Row():\n",
        "        gr.Markdown(\"### Админ-панель\")\n",
        "\n",
        "    with gr.Row():\n",
        "        # Model update column\n",
        "        with gr.Column():\n",
        "            model_link = gr.Textbox(label=\"Ссылка на модель (GGUF)\")\n",
        "            update_model_button = gr.Button(\"Обновить модель\")\n",
        "            update_model_output = gr.Textbox(label=\"Статус обновления модели\", interactive=False)\n",
        "            # Bind model update button\n",
        "            update_model_button.click(update_model, inputs=model_link, outputs=update_model_output)\n",
        "\n",
        "        # Context size column\n",
        "        with gr.Column():\n",
        "            context_size_input = gr.Textbox(label=\"Размер контекста (512-8192)\", value=str(CURRENT_CONTEXT_SIZE))\n",
        "            context_size_button = gr.Button(\"Изменить размер контекста\")\n",
        "            context_size_output = gr.Textbox(label=\"Статус изменения контекста\", interactive=False)\n",
        "            # Bind context size button\n",
        "            context_size_button.click(change_context_size, inputs=context_size_input, outputs=context_size_output)\n",
        "\n",
        "    with gr.Row():\n",
        "        # Temperature column\n",
        "        with gr.Column():\n",
        "            temperature_input = gr.Textbox(\n",
        "                label=\"Температура (0.0-2.0)\",\n",
        "                value=str(CURRENT_TEMPERATURE)\n",
        "            )\n",
        "            temperature_button = gr.Button(\"Изменить температуру\")\n",
        "            temperature_output = gr.Textbox(label=\"Статус изменения температуры\", interactive=False)\n",
        "            # Bind temperature button\n",
        "            temperature_button.click(change_temperature, inputs=temperature_input, outputs=temperature_output)\n",
        "\n",
        "        # RAG column\n",
        "        with gr.Column():\n",
        "            rag_button = gr.Button(\"Очистить и переинициализировать RAG\")\n",
        "            rag_output = gr.Textbox(label=\"Статус RAG\", interactive=False)\n",
        "            # Bind RAG button\n",
        "            rag_button.click(clear_rag, outputs=rag_output)\n",
        "\n",
        "    # --- Whisper settings ---\n",
        "    with gr.Row():\n",
        "        with gr.Column():\n",
        "            whisper_size = gr.Radio(\n",
        "                [\"tiny\", \"base\", \"small\", \"medium\", \"large\"],\n",
        "                label=\"Размер модели Whisper\",\n",
        "                value=\"small\"\n",
        "            )\n",
        "            whisper_update_button = gr.Button(\"Обновить модель Whisper\")\n",
        "            whisper_status = gr.Textbox(label=\"Статус Whisper\", interactive=False)\n",
        "\n",
        "            # Function to update Whisper model\n",
        "            def update_whisper_model(size):\n",
        "                try:\n",
        "                    global whisper_model\n",
        "                    whisper_model = load_whisper_model(size)\n",
        "                    return f\"Модель Whisper {size} успешно загружена\"\n",
        "                except Exception as e:\n",
        "                    return f\"Ошибка загрузки модели Whisper: {str(e)}\"\n",
        "\n",
        "            # Bind Whisper update button\n",
        "            whisper_update_button.click(update_whisper_model, inputs=whisper_size, outputs=whisper_status)\n",
        "\n",
        "    # Connect events\n",
        "    submit_button.click(submit_message_streaming, [chat_input, chatbot], [chatbot])\n",
        "    clear_button.click(lambda: ([], \"\"), outputs=[chatbot, chat_input])\n",
        "\n",
        "    # Connect Whisper processing to appropriate function based on mode\n",
        "    def process_audio_with_mode(audio, mode):\n",
        "        if mode == \"Заполнить поле ввода\":\n",
        "            transcription, _ = process_audio_and_fill_chat(audio)  # Правильно распаковываем результат\n",
        "            return transcription, chatbot\n",
        "        else:  # \"Сразу получить ответ\"\n",
        "            transcription = transcribe_audio(audio)\n",
        "            history = [(transcription, \"\")]\n",
        "\n",
        "            # Handle streaming generation\n",
        "            for h in submit_message_streaming(transcription, history):\n",
        "                history = h\n",
        "\n",
        "            return \"\", history\n",
        "\n",
        "    transcribe_button.click(\n",
        "        process_audio_with_mode,\n",
        "        inputs=[audio_input, whisper_mode],\n",
        "        outputs=[chat_input, chatbot]\n",
        "    )\n",
        "\n",
        "print(\"Интерфейс Gradio определен.\")\n",
        "\n",
        "# --- 3. Create FastAPI App ---\n",
        "print(\"Создание FastAPI приложения...\")\n",
        "api_app = FastAPI(title=\"Text Generation API\", description=\"API для генерации текста с использованием RAG\")\n",
        "print(\"FastAPI приложение создано.\")\n",
        "\n",
        "# --- 4. Define API Endpoints ---\n",
        "# Standard /api/ask endpoint\n",
        "@api_app.post(\"/api/ask\",\n",
        "            response_model=AnswerResponse,\n",
        "            summary=\"Задать вопрос модели\",\n",
        "            description=\"Отправляет вопрос модели и возвращает ответ.\")\n",
        "async def handle_ask_api(request: QuestionRequest):\n",
        "    print(f\"[API Request] /api/ask - Вопрос: {request.question}\")\n",
        "    try:\n",
        "        answer = ask_question_with_rag(request.question)\n",
        "        print(f\"[API Response] /api/ask - Ответ сгенерирован.\")\n",
        "        return AnswerResponse(answer=answer)\n",
        "    except Exception as e:\n",
        "        print(f\"[API Error] /api/ask - Ошибка: {str(e)}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        raise HTTPException(status_code=500, detail=f\"Ошибка генерации ответа: {str(e)}\")\n",
        "\n",
        "# New streaming endpoint\n",
        "from fastapi import FastAPI, Request\n",
        "import asyncio\n",
        "import logging\n",
        "from typing import Generator, AsyncGenerator\n",
        "\n",
        "# Enable more detailed logging\n",
        "logging.basicConfig(level=logging.DEBUG)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "@api_app.post(\"/api/ask/stream\",\n",
        "            summary=\"Упрощённый стриминг через SSE\",\n",
        "            description=\"Максимально простая реализация SSE для FastAPI\")\n",
        "async def handle_ask_stream_sse_simplified(request: QuestionRequest):\n",
        "    logger.info(f\"[API Request] /api/ask/stream - Вопрос: {request.question}\")\n",
        "\n",
        "    # Function to convert synchronous generator to async generator\n",
        "    async def async_generator_wrapper(sync_gen: Generator) -> AsyncGenerator:\n",
        "        for item in sync_gen:\n",
        "            # Format SSE message\n",
        "            formatted_item = f\"event: token\\ndata: {item}\\n\\n\"\n",
        "            yield formatted_item\n",
        "            # Small delay\n",
        "            await asyncio.sleep(0.01)\n",
        "        # Send done event at the end\n",
        "        yield \"event: done\\ndata: \\n\\n\"\n",
        "\n",
        "    # Try to get tokens from the model\n",
        "    try:\n",
        "        # Send initial comment to establish connection\n",
        "        async def stream_generator():\n",
        "            # Send initial empty data\n",
        "            yield \"data: \\n\\n\"\n",
        "\n",
        "            # Stream the tokens through the wrapper\n",
        "            async for chunk in async_generator_wrapper(ask_question_with_rag_stream(request.question)):\n",
        "                yield chunk\n",
        "\n",
        "    except Exception as e:\n",
        "        error_message = str(e)\n",
        "        logger.error(f\"Error in SSE generation: {error_message}\", exc_info=True)\n",
        "\n",
        "        # Return an error response\n",
        "        async def error_generator():\n",
        "            yield f\"data: \\n\\n\"\n",
        "            yield f\"event: error\\ndata: Ошибка: {error_message}\\n\\n\"\n",
        "\n",
        "        return StreamingResponse(\n",
        "            error_generator(),\n",
        "            media_type=\"text/event-stream\",\n",
        "            headers={\n",
        "                \"Cache-Control\": \"no-cache, no-transform\",\n",
        "                \"Connection\": \"keep-alive\",\n",
        "                \"X-Accel-Buffering\": \"no\"\n",
        "            }\n",
        "        )\n",
        "\n",
        "    # Return the successful response\n",
        "    return StreamingResponse(\n",
        "        stream_generator(),\n",
        "        media_type=\"text/event-stream\",\n",
        "        headers={\n",
        "            \"Cache-Control\": \"no-cache, no-transform\",\n",
        "            \"Connection\": \"keep-alive\",\n",
        "            \"X-Accel-Buffering\": \"no\"\n",
        "        }\n",
        "    )\n",
        "\n",
        "# New Whisper endpoint\n",
        "@api_app.post(\"/api/transcribe\",\n",
        "            summary=\"Транскрибировать аудио\",\n",
        "            description=\"Преобразует аудио в текст с помощью Whisper.\")\n",
        "async def transcribe_audio_api(file: UploadFile = File(...)):\n",
        "    try:\n",
        "        # Save the uploaded file temporarily\n",
        "        temp_file = tempfile.NamedTemporaryFile(delete=False, suffix=\".wav\")\n",
        "        temp_file.close()\n",
        "\n",
        "        # Write uploaded file content\n",
        "        with open(temp_file.name, \"wb\") as f:\n",
        "            f.write(await file.read())\n",
        "\n",
        "        # Transcribe the audio\n",
        "        transcribed_text = transcribe_audio(temp_file.name)\n",
        "\n",
        "        # Clean up\n",
        "        os.unlink(temp_file.name)\n",
        "\n",
        "        return {\"text\": transcribed_text}\n",
        "    except Exception as e:\n",
        "        raise HTTPException(status_code=500, detail=f\"Ошибка транскрибирования: {str(e)}\")\n",
        "\n",
        "# Combined endpoint: transcribe + generate answer\n",
        "@api_app.post(\"/api/speech-to-answer\",\n",
        "            summary=\"Получить ответ на голосовой вопрос\",\n",
        "            description=\"Преобразует аудио в текст и генерирует ответ.\")\n",
        "async def speech_to_answer_api(file: UploadFile = File(...)):\n",
        "    try:\n",
        "        # Save the uploaded file temporarily\n",
        "        temp_file = tempfile.NamedTemporaryFile(delete=False, suffix=\".wav\")\n",
        "        temp_file.close()\n",
        "\n",
        "        # Write uploaded file content\n",
        "        with open(temp_file.name, \"wb\") as f:\n",
        "            f.write(await file.read())\n",
        "\n",
        "        # Transcribe the audio\n",
        "        transcribed_text = transcribe_audio(temp_file.name)\n",
        "\n",
        "        # Clean up\n",
        "        os.unlink(temp_file.name)\n",
        "\n",
        "        # Generate answer\n",
        "        answer = ask_question_with_rag(transcribed_text)\n",
        "\n",
        "        return {\"transcription\": transcribed_text, \"answer\": answer}\n",
        "    except Exception as e:\n",
        "        raise HTTPException(status_code=500, detail=f\"Ошибка обработки речи: {str(e)}\")\n",
        "\n",
        "# --- 5. Mount Gradio App onto FastAPI ---\n",
        "print(\"Монтирование Gradio UI на FastAPI...\")\n",
        "api_app = gr.mount_gradio_app(api_app, demo, path=\"/\")\n",
        "print(\"Gradio UI смонтирован на FastAPI по пути '/'.\")\n",
        "\n",
        "# --- 6. Run with Uvicorn and Cloudflared Tunnel ---\n",
        "# Same as in your original code\n",
        "nest_asyncio.apply()\n",
        "\n",
        "async def run_server_and_tunnel():\n",
        "    config = uvicorn.Config(app=api_app, host=\"0.0.0.0\", port=7860, log_level=\"info\")\n",
        "    server = uvicorn.Server(config)\n",
        "    server_task = asyncio.create_task(server.serve())\n",
        "    print(\"Сервер Uvicorn запущен в фоновом режиме.\")\n",
        "    await asyncio.sleep(5)\n",
        "\n",
        "    print(\"Запуск cloudflared...\")\n",
        "    cf_process = subprocess.Popen(\n",
        "        ['cloudflared', 'tunnel', '--url', 'http://localhost:7860'],\n",
        "        stdout=subprocess.PIPE, stderr=subprocess.PIPE, universal_newlines=True\n",
        "    )\n",
        "\n",
        "    public_url = None\n",
        "    login_url_printed = False\n",
        "    start_time = time.time()\n",
        "    try:\n",
        "        while time.time() - start_time < 60:\n",
        "            line = cf_process.stderr.readline()\n",
        "            if not line and cf_process.poll() is not None: break\n",
        "            if not line:\n",
        "                await asyncio.sleep(0.5); continue\n",
        "            print(f\"[cloudflared] {line.strip()}\")\n",
        "\n",
        "            # Логин URL (первый раз)\n",
        "            if \"https://dash.cloudflare.com/argotunnel?callback=\" in line:\n",
        "                 login_url = re.search(r'(https://dash.cloudflare.com/argotunnel\\?callback=[^\\s]+)', line)\n",
        "                 if login_url:\n",
        "                      print(\"\\n\" + \"=\"*50 + \"\\n‼️ ТРЕБУЕТСЯ АУТЕНТИФИКАЦИЯ CLOUDFLARE (ОДИН РАЗ) ‼️\")\n",
        "                      print(f\"1. Скопируйте ссылку:\\n    {login_url.group(1)}\\n\")\n",
        "                      print(\"2. Откройте в браузере, войдите в Cloudflare, авторизуйте.\")\n",
        "                      print(\"3. После успеха в браузере, ОСТАНОВИТЕ и ПЕРЕЗАПУСТИТЕ эту ячейку.\\n\" + \"=\"*50 + \"\\n\")\n",
        "                      login_url_printed = True; cf_process.terminate(); await cf_process.wait(); return\n",
        "\n",
        "            # Готовый URL (после логина)\n",
        "            tunnel_url_match = re.search(r'(https://[a-zA-Z0-9-]+\\.trycloudflare\\.com)', line)\n",
        "            if tunnel_url_match:\n",
        "                public_url = tunnel_url_match.group(1)\n",
        "                print(\"\\n\" + \"=\"*50)\n",
        "                print(f\"✅ Публичный URL (Cloudflare Tunnel): {public_url}\")\n",
        "                print(f\"   API: {public_url}/api/ask | Docs: {public_url}/docs\")\n",
        "                print(\"--- Сервер и туннель активны. Остановите ячейку для завершения. ---\\n\" + \"=\"*50 + \"\\n\")\n",
        "                break # URL найден\n",
        "\n",
        "        if not public_url and not login_url_printed:\n",
        "             print(\"Не удалось получить URL от cloudflared за 60 секунд.\"); cf_process.terminate(); await cf_process.wait(); return\n",
        "\n",
        "        await server_task # Ждем завершения Uvicorn\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Ошибка: {e}\"); server_task.cancel()\n",
        "        try: await server_task\n",
        "        except asyncio.CancelledError: print(\"Сервер Uvicorn остановлен.\")\n",
        "    finally:\n",
        "        print(\"Завершение cloudflared...\");\n",
        "        if cf_process.poll() is None: # Проверяем, жив ли еще процесс\n",
        "             cf_process.terminate()\n",
        "             try:\n",
        "                 await asyncio.wait_for(asyncio.to_thread(cf_process.wait), timeout=5.0)\n",
        "             except asyncio.TimeoutError:\n",
        "                 print(\"Процесс cloudflared не завершился вовремя, убиваем...\")\n",
        "                 cf_process.kill()\n",
        "                 await asyncio.to_thread(cf_process.wait)\n",
        "        print(\"Процесс cloudflared завершен.\")\n",
        "\n",
        "\n",
        "# --- Запуск ---\n",
        "print(\"Запуск асинхронной функции для сервера и туннеля...\")\n",
        "asyncio.run(run_server_and_tunnel())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "import numpy as np\n",
        "\n",
        "# 1. Загрузка текста .docx\n",
        "def load_doc_content(doc_path):\n",
        "    from docx import Document\n",
        "    doc = Document(doc_path)\n",
        "    text = \"\\n\".join(p.text for p in doc.paragraphs)\n",
        "    return text\n",
        "\n",
        "# 2. Разбивка документа на фрагменты (по предложениям или абзацам)\n",
        "def split_doc_to_chunks(doc_text, min_length=20):\n",
        "    # Можно заменить на более сложный сплиттер при необходимости\n",
        "    return [chunk.strip() for chunk in doc_text.split('\\n') if len(chunk.strip()) >= min_length]\n",
        "\n",
        "# 3. Семантическая оценка (Sentence Transformers)\n",
        "def semantic_rag_usage(vector_db, doc_content, questions, k=3, model_name='paraphrase-multilingual-MiniLM-L12-v2', sim_threshold=0.6, show_details=True):\n",
        "    model = SentenceTransformer(model_name)\n",
        "    doc_chunks = split_doc_to_chunks(doc_content)\n",
        "    doc_embs = model.encode(doc_chunks)\n",
        "    used_count = 0\n",
        "    for q in questions:\n",
        "        answer = ask_question_with_rag(q)\n",
        "        answer_emb = model.encode([answer])[0]\n",
        "        similarities = np.dot(doc_embs, answer_emb) / (np.linalg.norm(doc_embs, axis=1) * np.linalg.norm(answer_emb) + 1e-8)\n",
        "        max_sim = np.max(similarities) if len(similarities) > 0 else 0\n",
        "        is_used = max_sim > sim_threshold\n",
        "        if show_details:\n",
        "            best_chunk = doc_chunks[np.argmax(similarities)] if len(similarities) > 0 else \"\"\n",
        "            print(f\"Q: {q}\\nModel Answer: {answer}\\nMax Similarity: {max_sim:.2f} | Семантическое совпадение: {is_used}\")\n",
        "            if is_used:\n",
        "                print(f\"Самый похожий фрагмент:\\n{best_chunk}\\n{'-'*40}\")\n",
        "            else:\n",
        "                print('-'*40)\n",
        "        if is_used:\n",
        "            used_count += 1\n",
        "    usage_rate = used_count / len(questions) if questions else 0\n",
        "    print(f\"\\nДоля ответов, семантически совпадающих с фрагментами документа: {usage_rate:.2f}\")\n",
        "    return usage_rate\n",
        "\n",
        "# ==== ПРИМЕР ИСПОЛЬЗОВАНИЯ ====\n",
        "doc_path = '/content/Doc/IRIT-RTF.docx'\n",
        "doc_content = load_doc_content(doc_path)\n",
        "questions = [\n",
        "    \"Сколько стоит обучение на направлении ИВТ?\",\n",
        "    \"Я изучаю C# и Python, какое направление мне стоит выбрать?\",\n",
        "    \"В чём разница между программной инженерией и прикладной информатикой?\",\n",
        "    \"Где находится приёмная комиссия?\",\n",
        "    \"Кем могут работать выпускники направления Радиотехника?\"\n",
        "]\n",
        "semantic_rag_usage(vector_db, doc_content, questions, k=3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xGsk21HbGtSP",
        "outputId": "ec23b779-0d46-4dda-f5da-01ec14ba3585"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q: Сколько стоит обучение на направлении ИВТ?\n",
            "Model Answer: Стоимость обучения на направлении «Информатика и вычислительная техника» составляет до 263 000 рублей.\n",
            "Max Similarity: 0.79 | Семантическое совпадение: True\n",
            "Самый похожий фрагмент:\n",
            "Стоимость обучения: До 263 000 ₽\n",
            "----------------------------------------\n",
            "Q: Я изучаю C# и Python, какое направление мне стоит выбрать?\n",
            "Model Answer: На основе предоставленных документов можно сделать вывод, что изучение языков программирования C# и Python входит в учебную программу Института вычислительной техники (ИВТ) Уральского Федерального университета. Вам может подойти образовательная программа, которая готовит специалистов в области разработки программного обеспечения. Вы сможете стать Back-разработчиком с вероятностью совпадения 95% или Front-разработчиком с вероятностью 85%. Также вы можете выбрать направление мобильного разработчика с вероятностью 85%.\n",
            "Max Similarity: 0.73 | Семантическое совпадение: True\n",
            "Самый похожий фрагмент:\n",
            "В процессе обучения студенты изучают архитектуру компьютерных сетей, методы защиты информации, инструменты мониторинга, тестирования и управления. В учебную программу также входят основы разработки программного обеспечения на языках программирования C#, Python, C++.\n",
            "----------------------------------------\n",
            "Q: В чём разница между программной инженерией и прикладной информатикой?\n",
            "Model Answer: К сожалению, в предоставленных документах нет информации о разнице между программной инженерией и прикладной информатикой. Вы можете ознакомиться с этой информацией на сайте УрФУ.\n",
            "Max Similarity: 0.41 | Семантическое совпадение: False\n",
            "----------------------------------------\n",
            "Q: Где находится приёмная комиссия?\n",
            "Model Answer: Приёмная комиссия ИРИТ-РТФ находится по адресу: г. Екатеринбург, ул. Мира, 32, Р-327.\n",
            "Max Similarity: 0.82 | Семантическое совпадение: True\n",
            "Самый похожий фрагмент:\n",
            "Приёмная по адресу : г. Екатеринбург, ул. Мира, 32, Р-327\n",
            "----------------------------------------\n",
            "Q: Кем могут работать выпускники направления Радиотехника?\n",
            "Model Answer: Выпускники направления «Радиотехника» могут работать в следующих профессиях:\n",
            "- Инженер-радиоэлектронщик в области радиотехники и телекоммуникаций (95%);\n",
            "- Инженер-проектировщик в области связи (телекоммуникаций) (90%);\n",
            "- Специалист в области аппаратно-программных средств цифровой обработки сигналов (90%);\n",
            "- Специалист в области радиоприёмных устройств (85%);\n",
            "- Специалист в области антенно-фидерных устройств радиотехнических средств и комплексов (80%);\n",
            "- Также выпускники могут быть специалистами в других областях, связанных с разработкой и проектированием радиоэлектронных средств и систем.\n",
            "Max Similarity: 0.83 | Семантическое совпадение: True\n",
            "Самый похожий фрагмент:\n",
            "Инженер-радиоэлектронщик в области радиотехники и телекоммуникаций: 95%\n",
            "----------------------------------------\n",
            "\n",
            "Доля ответов, семантически совпадающих с фрагментами документа: 0.80\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}