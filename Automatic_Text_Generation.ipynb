{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "После ошибки при Import Gradio нужно в Среде выполнения перезапустить сеанс и снова выполнить весь код"
      ],
      "metadata": {
        "id": "eoaubCx2nR8B"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LCL5pcWOr_Jj",
        "outputId": "51bba351-7240-4de0-d41d-ff63edd12434"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "libtesseract-dev is already the newest version (4.1.1-2.1build1).\n",
            "tesseract-ocr is already the newest version (4.1.1-2.1build1).\n",
            "tesseract-ocr-rus is already the newest version (1:4.00~git30-7274cfa-1.1).\n",
            "poppler-utils is already the newest version (22.02.0-2ubuntu0.8).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 34 not upgraded.\n"
          ]
        }
      ],
      "source": [
        "!apt-get -y install poppler-utils tesseract-ocr libtesseract-dev tesseract-ocr-rus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l3uZuhM0m3w3",
        "outputId": "f897d8ed-dec3-436b-ef91-f63329a69ab4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: gradio 5.29.0\n",
            "Uninstalling gradio-5.29.0:\n",
            "  Successfully uninstalled gradio-5.29.0\n"
          ]
        }
      ],
      "source": [
        "!pip uninstall -y gradio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "V1W40VHi8qBo"
      },
      "outputs": [],
      "source": [
        "# Cell 3 (Modified Again)\n",
        "!pip install -q numpy ctransformers[cuda] sentence-transformers chromadb langchain langchain-community langchain-huggingface gradio unstructured unstructured[pdf] unstructured[docx] python-docx fastapi uvicorn[standard] nest_asyncio pyngrok openai-whisper --quiet sse-starlette"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "8io1m_xTodiq"
      },
      "outputs": [],
      "source": [
        "# Install ctransformers with CPU support first (more reliable than GPU on some configurations)\n",
        "!pip install -q ctransformers --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "1b4KZvSE-bVW"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import gradio as gr\n",
        "import requests\n",
        "from ctransformers import AutoModelForCausalLM\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.document_loaders import DirectoryLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "\n",
        "# --- New API Imports ---\n",
        "import uvicorn\n",
        "from fastapi import FastAPI, HTTPException, Request\n",
        "from fastapi.responses import JSONResponse\n",
        "from pydantic import BaseModel\n",
        "import whisper\n",
        "import tempfile\n",
        "# --- End New API Imports ---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RWE_gUQYlClK",
        "outputId": "11d5f3f7-b01e-4fb0-e83d-3f70e848e03f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA available: True\n",
            "CUDA device: Tesla T4\n"
          ]
        }
      ],
      "source": [
        "# Check GPU availability\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "s3ISeNJLidf1"
      },
      "outputs": [],
      "source": [
        "# Model settings\n",
        "MODEL_URL = \"https://huggingface.co/mradermacher/YandexGPT-5-Lite-8B-instruct-GGUF/resolve/main/YandexGPT-5-Lite-8B-instruct.Q4_K_M.gguf\"\n",
        "MODEL_PATH = \"./YandexGPT-5-Lite-8B-instruct.Q4_K_M.gguf\"\n",
        "RAG_DIR = \"/content/rag_db\"\n",
        "CURRENT_CONTEXT_SIZE = 4096  # Default context size\n",
        "CURRENT_GPU_LAYERS = 24     # Default GPU layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "yYCvPIOgoPlj"
      },
      "outputs": [],
      "source": [
        "# Function to download model\n",
        "def download_model(url, save_path):\n",
        "    if os.path.exists(save_path):\n",
        "        return \"Модель уже загружена\"\n",
        "    try:\n",
        "        print(f\"Загрузка модели из {url}...\")\n",
        "        response = requests.get(url, stream=True)\n",
        "        response.raise_for_status()\n",
        "        total_size = int(response.headers.get('content-length', 0))\n",
        "        block_size = 8192\n",
        "        downloaded = 0\n",
        "\n",
        "        with open(save_path, \"wb\") as f:\n",
        "            for chunk in response.iter_content(chunk_size=block_size):\n",
        "                f.write(chunk)\n",
        "                downloaded += len(chunk)\n",
        "                if total_size > 0:\n",
        "                    print(f\"\\rЗагружено: {downloaded/1024/1024:.1f}MB / {total_size/1024/1024:.1f}MB ({downloaded*100/total_size:.1f}%)\", end=\"\")\n",
        "\n",
        "        print(\"\\nМодель успешно загружена\")\n",
        "        return \"Модель успешно загружена\"\n",
        "    except requests.RequestException as e:\n",
        "        return f\"Ошибка загрузки модели: {e}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "fQOVhtF9ijSI"
      },
      "outputs": [],
      "source": [
        "# Download model if needed\n",
        "if not os.path.exists(MODEL_PATH):\n",
        "    print(\"Скачивание модели...\")\n",
        "    download_model(MODEL_URL, MODEL_PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "nzr73Z6EQOic"
      },
      "outputs": [],
      "source": [
        "# Добавьте эту функцию перед функциями update_model и change_context_size\n",
        "\n",
        "def load_model_with_params(context_size=CURRENT_CONTEXT_SIZE, gpu_layers=CURRENT_GPU_LAYERS):\n",
        "    \"\"\"\n",
        "    Загружает модель с указанными параметрами размера контекста и количества GPU слоев.\n",
        "\n",
        "    Args:\n",
        "        context_size (int): Размер контекстного окна модели\n",
        "        gpu_layers (int): Количество слоев для выполнения на GPU\n",
        "\n",
        "    Returns:\n",
        "        tuple: (model, mode) - модель и режим работы (GPU/CPU)\n",
        "    \"\"\"\n",
        "    from ctransformers import AutoModelForCausalLM\n",
        "\n",
        "    try:\n",
        "        # Try with GPU support\n",
        "        print(f\"Загрузка модели с контекстом {context_size} и {gpu_layers} GPU слоями...\")\n",
        "        model = AutoModelForCausalLM.from_pretrained(\n",
        "            MODEL_PATH,\n",
        "            model_type=\"llama\",\n",
        "            gpu_layers=gpu_layers,\n",
        "            context_length=context_size,\n",
        "            batch_size=512\n",
        "        )\n",
        "        print(f\"Модель успешно загружена с контекстом {context_size}\")\n",
        "        return model, \"GPU\"\n",
        "    except Exception as e:\n",
        "        print(f\"Ошибка при загрузке модели с GPU: {e}\")\n",
        "        print(\"Загружаем модель в режиме CPU...\")\n",
        "\n",
        "        try:\n",
        "            model = AutoModelForCausalLM.from_pretrained(\n",
        "                MODEL_PATH,\n",
        "                model_type=\"llama\",\n",
        "                gpu_layers=0,  # CPU only mode\n",
        "                context_length=context_size,\n",
        "                batch_size=512\n",
        "            )\n",
        "            print(f\"Модель загружена в режиме CPU с контекстом {context_size}\")\n",
        "            return model, \"CPU\"\n",
        "        except Exception as e:\n",
        "            raise Exception(f\"Не удалось загрузить модель: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Whisper implementation\n",
        "whisper_model = None\n",
        "\n",
        "def load_whisper_model(model_size=\"small\"):\n",
        "    \"\"\"\n",
        "    Load Whisper model for speech recognition.\n",
        "\n",
        "    Args:\n",
        "        model_size (str): Size of the Whisper model to load.\n",
        "                    Options: \"tiny\", \"base\", \"small\", \"medium\", \"large\"\n",
        "\n",
        "    Returns:\n",
        "        The loaded Whisper model\n",
        "    \"\"\"\n",
        "    global whisper_model\n",
        "    try:\n",
        "        print(f\"Loading Whisper {model_size} model...\")\n",
        "        whisper_model = whisper.load_model(model_size)\n",
        "        print(f\"Whisper {model_size} model loaded successfully\")\n",
        "        return whisper_model\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading Whisper model: {e}\")\n",
        "        return None\n",
        "\n",
        "def transcribe_audio(audio_file):\n",
        "    \"\"\"\n",
        "    Transcribe audio file using Whisper model.\n",
        "\n",
        "    Args:\n",
        "        audio_file: Path to the audio file or audio file object\n",
        "\n",
        "    Returns:\n",
        "        str: Transcribed text\n",
        "    \"\"\"\n",
        "    global whisper_model\n",
        "\n",
        "    try:\n",
        "        # Load model if not loaded\n",
        "        if whisper_model is None:\n",
        "            whisper_model = load_whisper_model()\n",
        "            if whisper_model is None:\n",
        "                return \"Ошибка: Не удалось загрузить модель Whisper\"\n",
        "\n",
        "        # Handle file path or file object\n",
        "        temp_file = None\n",
        "        if isinstance(audio_file, str):\n",
        "            file_path = audio_file\n",
        "        else:\n",
        "            # Save to a temporary file if it's a file object\n",
        "            temp_file = tempfile.NamedTemporaryFile(delete=False, suffix=\".wav\")\n",
        "            temp_file.close()\n",
        "            audio_file.save(temp_file.name)\n",
        "            file_path = temp_file.name\n",
        "\n",
        "        # Transcribe audio\n",
        "        result = whisper_model.transcribe(file_path)\n",
        "        transcription = result[\"text\"].strip()\n",
        "\n",
        "        # Clean up temp file if created\n",
        "        if temp_file is not None:\n",
        "            os.unlink(temp_file.name)\n",
        "\n",
        "        return transcription\n",
        "    except Exception as e:\n",
        "        return f\"Ошибка транскрибирования аудио: {str(e)}\""
      ],
      "metadata": {
        "id": "iBTGEKPdZILM"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "gcy_vARplMxc"
      },
      "outputs": [],
      "source": [
        "# Create directories\n",
        "os.makedirs(\"/content/Doc\", exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "gsC97TA2dZ-5"
      },
      "outputs": [],
      "source": [
        "# Global variable to hold the sample document creation function\n",
        "def create_sample_document():\n",
        "    \"\"\"Create a sample document for RAG testing if none exists\"\"\"\n",
        "    sample_path = \"/content/Doc/sample.docx\"\n",
        "    try:\n",
        "        from docx import Document\n",
        "        doc = Document()\n",
        "        doc.add_paragraph(\"Этот документ создан для примера работы системы RAG с УрФУ.\")\n",
        "        doc.add_paragraph(\"Уральский федеральный университет (УрФУ) расположен в Екатеринбурге.\")\n",
        "        doc.add_paragraph(\"УрФУ является одним из ведущих вузов России.\")\n",
        "        doc.add_paragraph(\"В УрФУ обучаются студенты со всей России и из многих зарубежных стран.\")\n",
        "        doc.add_paragraph(\"УрФУ предлагает программы бакалавриата, магистратуры и аспирантуры.\")\n",
        "        doc.save(sample_path)\n",
        "        print(f\"✅ Создан пример документа для тестирования: {sample_path}\")\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Ошибка при создании примера документа: {e}\")\n",
        "        return False\n",
        "\n",
        "# RAG functions with improved error handling\n",
        "def initialize_rag():\n",
        "    \"\"\"Initialize the RAG database with documents\"\"\"\n",
        "    try:\n",
        "        # Ensure the directory exists\n",
        "        os.makedirs(\"/content/Doc\", exist_ok=True)\n",
        "\n",
        "        # Check if there are any documents\n",
        "        doc_files = [f for f in os.listdir(\"/content/Doc\") if f.endswith(\".docx\")]\n",
        "\n",
        "        if not doc_files:\n",
        "            print(\"Нет документов для загрузки. Создаем пример документа...\")\n",
        "            created = create_sample_document()\n",
        "            if not created:\n",
        "                print(\"Не удалось создать пример документа. Инициализация RAG не выполнена.\")\n",
        "                return None\n",
        "\n",
        "        # Load documents\n",
        "        loader = DirectoryLoader(\"/content/Doc\", glob=\"**/*.docx\")\n",
        "        documents = loader.load()\n",
        "\n",
        "        if not documents:\n",
        "            print(\"Не удалось загрузить документы, даже после создания примера.\")\n",
        "            print(\"Проверьте, что папка /content/Doc содержит доступные файлы .docx\")\n",
        "            return None\n",
        "\n",
        "        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "        texts = text_splitter.split_documents(documents)\n",
        "\n",
        "        if not texts:\n",
        "            print(\"Документы загружены, но не удалось извлечь текст.\")\n",
        "            return None\n",
        "\n",
        "        embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\")\n",
        "        vector_db = Chroma.from_documents(texts, embeddings, persist_directory=RAG_DIR)\n",
        "        print(f\"✅ База RAG успешно инициализирована с {len(texts)} фрагментами\")\n",
        "        return vector_db\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Ошибка инициализации RAG: {e}\")\n",
        "        return None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MU84NQmGoZsl",
        "outputId": "3a7512cb-4de7-4928-92e6-18be02265e26"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Загружаем существующую базу RAG...\n",
            "✅ База RAG успешно загружена\n"
          ]
        }
      ],
      "source": [
        "# Load or initialize RAG database with better error handling\n",
        "vector_db = None  # Initialize to None first\n",
        "try:\n",
        "    if not os.path.exists(RAG_DIR):\n",
        "        print(\"RAG база данных не найдена, создаем новую...\")\n",
        "        vector_db = initialize_rag()\n",
        "        if vector_db is None:\n",
        "            print(\"❌ Не удалось инициализировать RAG. Ответы модели не будут использовать контекст документов.\")\n",
        "    else:\n",
        "        try:\n",
        "            print(\"Загружаем существующую базу RAG...\")\n",
        "            embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\")\n",
        "            vector_db = Chroma(persist_directory=RAG_DIR, embedding_function=embeddings)\n",
        "            print(\"✅ База RAG успешно загружена\")\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Ошибка загрузки существующей базы RAG: {e}\")\n",
        "            print(\"Пробуем создать новую базу RAG...\")\n",
        "            if os.path.exists(RAG_DIR):\n",
        "                import shutil\n",
        "                shutil.rmtree(RAG_DIR)\n",
        "            vector_db = initialize_rag()\n",
        "            if vector_db is None:\n",
        "                print(\"❌ Не удалось инициализировать RAG. Ответы модели не будут использовать контекст документов.\")\n",
        "except Exception as e:\n",
        "    print(f\"❌ Неожиданная ошибка при работе с RAG: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "JxfLy4ntbGML"
      },
      "outputs": [],
      "source": [
        "# Фиксированный контекст, который будет передаваться модели\n",
        "FIXED_CONTEXT = \"\"\"Ты — интеллектуальный помощник, обученный отвечать на вопросы строго в рамках предоставленного контекста. Ты помощник по Уральскому Федеральному университету, который общается с пользователем. Общайся с ним только на русском языке. Объясняй максимально подробно, чтобы пользователь всё понял. Если информации недостаточно, отвечай \"Вы можете ознакомиться с данной информацией на сайте УрФУ\". Избегай выдумок и предположений. Если пользователь просит дать точные значения - обращайся только к данным RAG.\"\"\"\n",
        "CURRENT_TEMPERATURE = 0.5  # Значение по умолчанию\n",
        "# Function to generate response using RAG\n",
        "# Обновленная функция для генерации ответов с улучшенным контролем источников\n",
        "def ask_question_with_rag(question):\n",
        "    try:\n",
        "        # Check if vector_db is available\n",
        "        if vector_db is None:\n",
        "            # Fallback to model-only generation\n",
        "            prompt = f\"{FIXED_CONTEXT}\\n\\nUser: {question}\\nAssistant:\"\n",
        "            response = model(\n",
        "                prompt,\n",
        "                max_new_tokens=256,\n",
        "                temperature=CURRENT_TEMPERATURE,\n",
        "                stop=[\"User:\", \"\\n\\n\"]\n",
        "            )\n",
        "            return response.strip() + \"\\n\\n(Примечание: ответ дан без использования базы знаний, так как RAG не инициализирован)\"\n",
        "\n",
        "        # Retrieve relevant documents\n",
        "        docs = vector_db.similarity_search(question, k=3)\n",
        "\n",
        "        # Check if the retrieved documents are actually relevant by looking for keywords\n",
        "        query_keywords = set(question.lower().split())\n",
        "        important_keywords = {word for word in query_keywords\n",
        "                             if len(word) > 3 and word not in\n",
        "                             ['что', 'как', 'где', 'когда', 'какие', 'какой', 'какая', 'институт', 'урфу', 'университет', 'ИРИТ-РТФ', 'РТФ']}\n",
        "\n",
        "        # Extract and show relevant context\n",
        "        extracted_context = \"\\n\\n\".join([doc.page_content for doc in docs])\n",
        "\n",
        "        # Check if any important keywords are in the context\n",
        "        found_keywords = []\n",
        "        for keyword in important_keywords:\n",
        "            if keyword in extracted_context.lower():\n",
        "                found_keywords.append(keyword)\n",
        "\n",
        "        # Determine relevance score\n",
        "        relevance_score = len(found_keywords) / max(1, len(important_keywords)) if important_keywords else 0.5\n",
        "\n",
        "        # Add strong instruction about only using provided context\n",
        "        strict_instruction = \"\"\"\n",
        "ВАЖНО: Отвечай ТОЛЬКО на основе предоставленной информации из документов.\n",
        "Если в предоставленных документах нет ответа на вопрос, честно скажи\n",
        "\"В документах нет информации о [тема вопроса]. Вы можете ознакомиться с этой информацией на сайте УрФУ.\"\n",
        "НЕ ПРИДУМЫВАЙ информацию, которой нет в документах!\n",
        "\"\"\"\n",
        "        combined_context = f\"{FIXED_CONTEXT}\\n\\n{strict_instruction}\\n\\nДокументы:\\n{extracted_context}\"\n",
        "\n",
        "        # Create prompt for model with stronger guidance\n",
        "        prompt = f\"Context: {combined_context}\\n\\nUser: {question}\\n\\nAssistant:\"\n",
        "\n",
        "        # Use even lower temperature for low relevance scores to reduce hallucination\n",
        "        adjusted_temperature = min(CURRENT_TEMPERATURE, 0.3) if relevance_score < 0.5 else CURRENT_TEMPERATURE\n",
        "\n",
        "        # Generate response with ctransformers\n",
        "        response = model(\n",
        "            prompt,\n",
        "            max_new_tokens=256,\n",
        "            temperature=adjusted_temperature,  # Use adjusted temperature based on relevance\n",
        "            stop=[\"User:\", \"\\n\\n\"]\n",
        "        )\n",
        "\n",
        "        # Return the generated text with optional debugging info\n",
        "        result = response.strip()\n",
        "\n",
        "        # For debugging - uncomment to show relevance information\n",
        "        # debug_info = f\"\\n\\n[Отладка: Найдено {len(found_keywords)}/{len(important_keywords)} ключевых слов, релевантность {relevance_score:.2f}]\"\n",
        "        # return result + debug_info\n",
        "\n",
        "        return result\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"Произошла ошибка при генерации ответа: {str(e)}\\n\\nПожалуйста, попробуйте очистить и переинициализировать RAG.\"\n",
        "\n",
        "# Function to update model\n",
        "def update_model(link):\n",
        "    try:\n",
        "        global model, mode\n",
        "        result = download_model(link, MODEL_PATH)\n",
        "        # Reload with current context size\n",
        "        model, mode = load_model_with_params(CURRENT_CONTEXT_SIZE, CURRENT_GPU_LAYERS)\n",
        "        return f\"Модель обновлена: {result}. Режим работы: {mode}\"\n",
        "    except Exception as e:\n",
        "        return f\"Ошибка при обновлении модели: {str(e)}\"\n",
        "\n",
        "# Function to change context size\n",
        "def change_context_size(new_size_str):\n",
        "    try:\n",
        "        global model, CURRENT_CONTEXT_SIZE, mode\n",
        "\n",
        "        # Convert to integer and validate\n",
        "        new_size = int(new_size_str)\n",
        "        if new_size < 512:\n",
        "            return \"Ошибка: размер контекста должен быть не менее 512\"\n",
        "        if new_size > 8192:\n",
        "            return \"Ошибка: размер контекста не может превышать 8192\"\n",
        "\n",
        "        CURRENT_CONTEXT_SIZE = new_size\n",
        "        model, mode = load_model_with_params(CURRENT_CONTEXT_SIZE, CURRENT_GPU_LAYERS)\n",
        "        return f\"Размер контекста изменен на {new_size}. Режим работы: {mode}\"\n",
        "    except ValueError:\n",
        "        return \"Ошибка: введите корректное целое число\"\n",
        "    except Exception as e:\n",
        "        return f\"Ошибка при изменении размера контекста: {str(e)}\"\n",
        "\n",
        "def change_temperature(new_temp_str):\n",
        "    try:\n",
        "        global CURRENT_TEMPERATURE\n",
        "\n",
        "        # Convert to float and validate\n",
        "        new_temp = float(new_temp_str)\n",
        "        if new_temp < 0.0:\n",
        "            return \"Ошибка: температура не может быть меньше 0.0\"\n",
        "        if new_temp > 2.0:\n",
        "            return \"Ошибка: температура не рекомендуется выше 2.0\"\n",
        "\n",
        "        CURRENT_TEMPERATURE = new_temp\n",
        "        return f\"Температура генерации изменена на {new_temp}\"\n",
        "    except ValueError:\n",
        "        return \"Ошибка: введите корректное число с плавающей точкой (например, 0.2)\"\n",
        "    except Exception as e:\n",
        "        return f\"Ошибка при изменении температуры: {str(e)}\"\n",
        "\n",
        "# Function to clear RAG\n",
        "def clear_rag():\n",
        "    try:\n",
        "        if os.path.exists(RAG_DIR):\n",
        "            import shutil\n",
        "            shutil.rmtree(RAG_DIR)\n",
        "        global vector_db\n",
        "        vector_db = initialize_rag()\n",
        "        return \"RAG очищен и переинициализирован\"\n",
        "    except Exception as e:\n",
        "        return f\"Ошибка при очистке RAG: {str(e)}\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Streaming version of question answering\n",
        "def ask_question_with_rag_stream(question):\n",
        "    \"\"\"\n",
        "    Streaming version of ask_question_with_rag\n",
        "\n",
        "    Args:\n",
        "        question (str): The question to answer\n",
        "\n",
        "    Yields:\n",
        "        str: Generated text tokens\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Check if vector_db is available\n",
        "        if vector_db is None:\n",
        "            # Fallback to model-only generation\n",
        "            prompt = f\"{FIXED_CONTEXT}\\n\\nUser: {question}\\nAssistant:\"\n",
        "\n",
        "            # Generate tokens\n",
        "            for token in model(\n",
        "                prompt,\n",
        "                max_new_tokens=256,\n",
        "                temperature=CURRENT_TEMPERATURE,\n",
        "                stop=[\"User:\", \"\\n\\n\"],\n",
        "                stream=True\n",
        "            ):\n",
        "                yield token\n",
        "\n",
        "            yield \"\\n\\n(Примечание: ответ дан без использования базы знаний, так как RAG не инициализирован)\"\n",
        "            return\n",
        "\n",
        "        # Retrieve relevant documents\n",
        "        docs = vector_db.similarity_search(question, k=3)\n",
        "\n",
        "        # Check if retrieved documents are actually relevant by looking for keywords\n",
        "        query_keywords = set(question.lower().split())\n",
        "        important_keywords = {word for word in query_keywords\n",
        "                         if len(word) > 3 and word not in\n",
        "                         ['что', 'как', 'где', 'когда', 'какие', 'какой', 'какая', 'институт', 'урфу', 'университет', 'ИРИТ-РТФ', 'РТФ']}\n",
        "\n",
        "        # Extract context\n",
        "        extracted_context = \"\\n\\n\".join([doc.page_content for doc in docs])\n",
        "\n",
        "        # Check if any important keywords are in the context\n",
        "        found_keywords = []\n",
        "        for keyword in important_keywords:\n",
        "            if keyword in extracted_context.lower():\n",
        "                found_keywords.append(keyword)\n",
        "\n",
        "        # Determine relevance score\n",
        "        relevance_score = len(found_keywords) / max(1, len(important_keywords)) if important_keywords else 0.5\n",
        "\n",
        "        # Add strong instruction about only using provided context\n",
        "        strict_instruction = \"\"\"\n",
        "ВАЖНО: Отвечай ТОЛЬКО на основе предоставленной информации из документов.\n",
        "Если в предоставленных документах нет ответа на вопрос, честно скажи\n",
        "\"В документах нет информации о [тема вопроса]. Вы можете ознакомиться с этой информацией на сайте УрФУ.\"\n",
        "НЕ ПРИДУМЫВАЙ информацию, которой нет в документах!\n",
        "\"\"\"\n",
        "        combined_context = f\"{FIXED_CONTEXT}\\n\\n{strict_instruction}\\n\\nДокументы:\\n{extracted_context}\"\n",
        "\n",
        "        # Create prompt for model with stronger guidance\n",
        "        prompt = f\"Context: {combined_context}\\n\\nUser: {question}\\n\\nAssistant:\"\n",
        "\n",
        "        # Use lower temperature for low relevance scores to reduce hallucination\n",
        "        adjusted_temperature = min(CURRENT_TEMPERATURE, 0.3) if relevance_score < 0.5 else CURRENT_TEMPERATURE\n",
        "\n",
        "        # Generate response with ctransformers in streaming mode\n",
        "        for token in model(\n",
        "            prompt,\n",
        "            max_new_tokens=256,\n",
        "            temperature=adjusted_temperature,\n",
        "            stop=[\"User:\", \"\\n\\n\"],\n",
        "            stream=True\n",
        "        ):\n",
        "            yield token\n",
        "\n",
        "    except Exception as e:\n",
        "        yield f\"Произошла ошибка при генерации ответа: {str(e)}\\n\\nПожалуйста, попробуйте очистить и переинициализировать RAG.\""
      ],
      "metadata": {
        "id": "MD6zCOibZdNs"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "fTOCtgJu4vCO"
      },
      "outputs": [],
      "source": [
        "# Cell 14.1 (New) - API Models\n",
        "class QuestionRequest(BaseModel):\n",
        "    question: str\n",
        "\n",
        "class AnswerResponse(BaseModel):\n",
        "    answer: str"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TjtfQVuu8J3t",
        "outputId": "680963b8-1eb2-4f50-8889-7e285173cc40"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Установка cloudflared...\n",
            "cloudflared установлен.\n"
          ]
        }
      ],
      "source": [
        "# Cell 14.5 (New) - Install cloudflared\n",
        "print(\"Установка cloudflared...\")\n",
        "!wget -q https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64 -O cloudflared\n",
        "!chmod +x cloudflared\n",
        "print(\"cloudflared установлен.\")\n",
        "# Переместим в /usr/local/bin для удобства вызова (не обязательно, но рекомендуется)\n",
        "!mv cloudflared /usr/local/bin/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NIBY23V2CIZk",
        "outputId": "d31fd22a-f709-45d9-b59d-4f4fd2860c03"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Загрузка модели...\n",
            "Загрузка модели с контекстом 4096 и 24 GPU слоями...\n",
            "Модель успешно загружена с контекстом 4096\n",
            "Модель загружена. Режим: GPU\n",
            "Loading Whisper small model...\n",
            "Whisper small model loaded successfully\n",
            "Определение интерфейса Gradio (Чат + Админ-панель + Whisper)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-47-b858432dd328>:66: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
            "  chatbot = gr.Chatbot(label=\"История чата\", height=400)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Интерфейс Gradio определен.\n",
            "Создание FastAPI приложения...\n",
            "FastAPI приложение создано.\n",
            "Монтирование Gradio UI на FastAPI...\n",
            "Gradio UI смонтирован на FastAPI по пути '/'.\n",
            "Запуск асинхронной функции для сервера и туннеля...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:     Started server process [9605]\n",
            "INFO:     Waiting for application startup.\n",
            "INFO:     Application startup complete.\n",
            "INFO:     Uvicorn running on http://0.0.0.0:7860 (Press CTRL+C to quit)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Сервер Uvicorn запущен в фоновом режиме.\n",
            "Запуск cloudflared...\n",
            "[cloudflared] 2025-05-06T15:13:07Z INF Thank you for trying Cloudflare Tunnel. Doing so, without a Cloudflare account, is a quick way to experiment and try it out. However, be aware that these account-less Tunnels have no uptime guarantee, are subject to the Cloudflare Online Services Terms of Use (https://www.cloudflare.com/website-terms/), and Cloudflare reserves the right to investigate your use of Tunnels for violations of such terms. If you intend to use Tunnels in production you should use a pre-created named tunnel by following: https://developers.cloudflare.com/cloudflare-one/connections/connect-apps\n",
            "[cloudflared] 2025-05-06T15:13:07Z INF Requesting new quick Tunnel on trycloudflare.com...\n",
            "[cloudflared] 2025-05-06T15:13:12Z INF +--------------------------------------------------------------------------------------------+\n",
            "[cloudflared] 2025-05-06T15:13:12Z INF |  Your quick Tunnel has been created! Visit it at (it may take some time to be reachable):  |\n",
            "[cloudflared] 2025-05-06T15:13:12Z INF |  https://failure-medline-samba-spend.trycloudflare.com                                     |\n",
            "\n",
            "==================================================\n",
            "✅ Публичный URL (Cloudflare Tunnel): https://failure-medline-samba-spend.trycloudflare.com\n",
            "   API: https://failure-medline-samba-spend.trycloudflare.com/api/ask | Docs: https://failure-medline-samba-spend.trycloudflare.com/docs\n",
            "--- Сервер и туннель активны. Остановите ячейку для завершения. ---\n",
            "==================================================\n",
            "\n",
            "INFO:     221.132.29.208:0 - \"GET /docs HTTP/1.1\" 200 OK\n",
            "INFO:     34.83.203.92:0 - \"GET /docs HTTP/1.1\" 200 OK\n",
            "INFO:     34.83.203.92:0 - \"GET /docs HTTP/1.1\" 200 OK\n",
            "INFO:     34.83.203.92:0 - \"GET /docs HTTP/1.1\" 200 OK\n",
            "INFO:     34.83.203.92:0 - \"GET /docs HTTP/1.1\" 200 OK\n",
            "INFO:     34.83.203.92:0 - \"GET /openapi.json HTTP/1.1\" 200 OK\n",
            "INFO:     34.83.203.92:0 - \"GET /openapi.json HTTP/1.1\" 200 OK\n",
            "INFO:     34.83.203.92:0 - \"GET /favicon.ico HTTP/1.1\" 200 OK\n",
            "INFO:     221.132.29.208:0 - \"GET /openapi.json HTTP/1.1\" 200 OK\n",
            "INFO:     221.132.29.208:0 - \"POST /api/ask/stream HTTP/1.1\" 200 OK\n",
            "INFO:     5.165.12.71:0 - \"POST /api/ask/stream HTTP/1.1\" 200 OK\n",
            "INFO:     221.132.29.208:0 - \"GET / HTTP/1.1\" 307 Temporary Redirect\n",
            "INFO:     221.132.29.208:0 - \"GET // HTTP/1.1\" 200 OK\n",
            "INFO:     221.132.29.208:0 - \"GET //assets/index-Do3LSwBC.js HTTP/1.1\" 200 OK\n",
            "INFO:     221.132.29.208:0 - \"GET //assets/index-CHc6r96-.css HTTP/1.1\" 200 OK\n",
            "INFO:     221.132.29.208:0 - \"GET //assets/svelte/svelte.js HTTP/1.1\" 200 OK\n",
            "INFO:     221.132.29.208:0 - \"GET //assets/IconButtonWrapper.svelte_svelte_type_style_lang-Dn324h3u.js HTTP/1.1\" 200 OK\n",
            "INFO:     221.132.29.208:0 - \"GET //assets/StreamingBar-DOagx4HU.css HTTP/1.1\" 200 OK\n",
            "INFO:     221.132.29.208:0 - \"GET //assets/index-Dk7BRz4q.js HTTP/1.1\" 200 OK\n",
            "INFO:     221.132.29.208:0 - \"GET //assets/MarkdownCode.svelte_svelte_type_style_lang-C4fBIMNi.js HTTP/1.1\" 200 OK\n",
            "INFO:     221.132.29.208:0 - \"GET //assets/IconButtonWrapper-BoUZMnMb.css HTTP/1.1\" 200 OK\n",
            "INFO:     221.132.29.208:0 - \"GET //assets/StreamingBar.svelte_svelte_type_style_lang-CDNxkBIr.js HTTP/1.1\" 200 OK\n",
            "INFO:     221.132.29.208:0 - \"GET //assets/Index-Cuq53rtW.js HTTP/1.1\" 200 OK\n",
            "INFO:     221.132.29.208:0 - \"GET //assets/Embed-CF_bgppJ.js HTTP/1.1\" 200 OK\n",
            "INFO:     221.132.29.208:0 - \"GET //assets/MarkdownCode-PT0nGY9v.css HTTP/1.1\" 200 OK\n",
            "INFO:     221.132.29.208:0 - \"GET //assets/DownloadLink.svelte_svelte_type_style_lang-C_5UIfol.js HTTP/1.1\" 200 OK\n",
            "INFO:     221.132.29.208:0 - \"GET //assets/DownloadLink-CqD3Uu0l.css HTTP/1.1\" 200 OK\n",
            "INFO:     221.132.29.208:0 - \"GET //assets/Clear-By3xiIwg.js HTTP/1.1\" 200 OK\n",
            "INFO:     221.132.29.208:0 - \"GET //assets/prism-python-Cp7Xa0u5.js HTTP/1.1\" 200 OK\n",
            "INFO:     221.132.29.208:0 - \"GET //assets/Index-BJ_RfjVB.css HTTP/1.1\" 200 OK\n",
            "INFO:     221.132.29.208:0 - \"GET //assets/IconButton-DSs-aBrk.js HTTP/1.1\" 200 OK\n",
            "INFO:     221.132.29.208:0 - \"GET /favicon.ico HTTP/1.1\" 200 OK\n",
            "INFO:     221.132.29.208:0 - \"GET /manifest.json HTTP/1.1\" 404 Not Found\n",
            "INFO:     221.132.29.208:0 - \"GET //assets/Example-D7K5RtQ2.css HTTP/1.1\" 200 OK\n",
            "INFO:     221.132.29.208:0 - \"GET //assets/Info-B6Y9MYYe.js HTTP/1.1\" 200 OK\n",
            "INFO:     221.132.29.208:0 - \"GET //assets/Blocks-BMC4HgbM.js HTTP/1.1\" 200 OK\n",
            "INFO:     221.132.29.208:0 - \"GET //assets/Dropdown-CWxB-qJp.css HTTP/1.1\" 200 OK\n",
            "INFO:     221.132.29.208:0 - \"GET //assets/DropdownArrow-CFmkDyPW.js HTTP/1.1\" 200 OK\n",
            "INFO:     221.132.29.208:0 - \"GET //assets/MarkdownCode-qms8blzp.js HTTP/1.1\" 200 OK\n",
            "INFO:     221.132.29.208:0 - \"GET //assets/Toast-7aPQ8_ya.js HTTP/1.1\" 200 OK\n",
            "INFO:     221.132.29.208:0 - \"GET //assets/Image-CnqB5dbD.js HTTP/1.1\" 200 OK\n",
            "INFO:     221.132.29.208:0 - \"GET //assets/file-url-DoxvUUVV.js HTTP/1.1\" 200 OK\n",
            "INFO:     221.132.29.208:0 - \"GET //assets/Button-DTh9AgeE.css HTTP/1.1\" 200 OK\n",
            "INFO:     221.132.29.208:0 - \"GET //assets/Block-CygNNd5z.js HTTP/1.1\" 200 OK\n",
            "INFO:     221.132.29.208:0 - \"GET //assets/Image-B8dFOee4.css HTTP/1.1\" 200 OK\n",
            "INFO:     221.132.29.208:0 - \"GET //assets/index-KfiBD3Sx.js HTTP/1.1\" 200 OK\n",
            "INFO:     221.132.29.208:0 - \"GET //assets/Blocks-CVEQURsJ.css HTTP/1.1\" 200 OK\n",
            "INFO:     221.132.29.208:0 - \"GET //assets/ImagePreview-C_qhEOxI.css HTTP/1.1\" 200 OK\n",
            "INFO:     221.132.29.208:0 - \"GET //assets/Dropdown-DSZkNuau.js HTTP/1.1\" 200 OK\n",
            "INFO:     221.132.29.208:0 - \"GET //assets/BlockTitle-Bznni5NZ.js HTTP/1.1\" 200 OK\n",
            "INFO:     221.132.29.208:0 - \"GET //assets/utils-BsGrhMNe.js HTTP/1.1\" 200 OK\n",
            "INFO:     221.132.29.208:0 - \"GET /theme.css?v=63194d3741d384f9f85db890247b6c0ef9e7abac0f297f40a15c59fe4baba916 HTTP/1.1\" 200 OK\n",
            "INFO:     221.132.29.208:0 - \"GET //assets/Button-D62RTh6z.js HTTP/1.1\" 200 OK\n",
            "INFO:     221.132.29.208:0 - \"GET /static/fonts/ui-sans-serif/ui-sans-serif-Regular.woff2 HTTP/1.1\" 404 Not Found\n",
            "INFO:     221.132.29.208:0 - \"GET //assets/Index-Bd93DQPF.js HTTP/1.1\" 200 OK\n",
            "INFO:     221.132.29.208:0 - \"GET //assets/Copy-CxQ9EyK2.js HTTP/1.1\" 200 OK\n",
            "INFO:     221.132.29.208:0 - \"GET //assets/Index-D-Rt2GyV.js HTTP/1.1\" 200 OK\n",
            "INFO:     221.132.29.208:0 - \"GET //assets/InteractiveAudio-B76TQFG-.css HTTP/1.1\" 200 OK\n",
            "INFO:     221.132.29.208:0 - \"GET //assets/AudioPlayer-BAKhejK8.css HTTP/1.1\" 200 OK\n",
            "INFO:     221.132.29.208:0 - \"GET //assets/Index-DXEfh5mS.js HTTP/1.1\" 200 OK\n",
            "INFO:     221.132.29.208:0 - \"GET //assets/Undo-DCjBnnSO.js HTTP/1.1\" 200 OK\n",
            "INFO:     221.132.29.208:0 - \"GET //assets/Index-7U9UAML0.css HTTP/1.1\" 200 OK\n",
            "INFO:     221.132.29.208:0 - \"GET //assets/Music-CDm0RGMk.js HTTP/1.1\" 200 OK\n",
            "INFO:     221.132.29.208:0 - \"GET //assets/Example-Ce48OhaN.js HTTP/1.1\" 200 OK\n",
            "INFO:     221.132.29.208:0 - \"GET //assets/Index-12OnbRhk.css HTTP/1.1\" 200 OK\n",
            "INFO:     221.132.29.208:0 - \"GET //assets/Edit-BpRIf5rU.js HTTP/1.1\" 200 OK\n",
            "INFO:     221.132.29.208:0 - \"GET //assets/File-BQ_9P3Ye.js HTTP/1.1\" 200 OK\n",
            "INFO:     221.132.29.208:0 - \"GET //assets/Index--UpFQsHg.css HTTP/1.1\" 200 OK\n",
            "INFO:     221.132.29.208:0 - \"GET //assets/Empty-AVq7jL0F.js HTTP/1.1\" 200 OK\n",
            "INFO:     221.132.29.208:0 - \"GET //assets/IconButtonWrapper-jknaAjP6.js HTTP/1.1\" 200 OK\n",
            "INFO:     221.132.29.208:0 - \"GET //assets/AudioPlayer-DG1QBBp6.js HTTP/1.1\" 200 OK\n",
            "INFO:     221.132.29.208:0 - \"GET //assets/Index-CfowPFmo.css HTTP/1.1\" 200 OK\n",
            "INFO:     221.132.29.208:0 - \"GET //assets/Upload-L7mprsyN.css HTTP/1.1\" 200 OK\n",
            "INFO:     221.132.29.208:0 - \"GET //assets/index-CnqicUFC.js HTTP/1.1\" 200 OK\n",
            "INFO:     221.132.29.208:0 - \"GET //assets/Index-Ru4sKdXi.css HTTP/1.1\" 200 OK\n",
            "INFO:     221.132.29.208:0 - \"GET //assets/ShareButton-CuWfx-Ef.js HTTP/1.1\" 200 OK\n",
            "INFO:     221.132.29.208:0 - \"GET //assets/Index-9DRrr-z6.js HTTP/1.1\" 200 OK\n",
            "INFO:     221.132.29.208:0 - \"GET //assets/Play-B0Q0U1Qz.js HTTP/1.1\" 200 OK\n",
            "INFO:     221.132.29.208:0 - \"GET //assets/BlockLabel-8WtM61r3.js HTTP/1.1\" 200 OK\n",
            "INFO:     221.132.29.208:0 - \"GET //assets/Trash-RbZEwH-j.js HTTP/1.1\" 200 OK\n",
            "INFO:     221.132.29.208:0 - \"GET //assets/SelectSource-Ce_27dAp.js HTTP/1.1\" 200 OK\n",
            "INFO:     221.132.29.208:0 - \"GET //assets/index-Cy7ZTydd.js HTTP/1.1\" 200 OK\n",
            "INFO:     221.132.29.208:0 - \"GET //assets/Send-DyoOovnk.js HTTP/1.1\" 200 OK\n",
            "INFO:     221.132.29.208:0 - \"GET //assets/Community-Dw1micSV.js HTTP/1.1\" 200 OK\n",
            "INFO:     221.132.29.208:0 - \"GET //assets/StaticAudio-NnvWX8qs.js HTTP/1.1\" 200 OK\n",
            "INFO:     221.132.29.208:0 - \"GET //assets/Check-CEkiXcyC.js HTTP/1.1\" 200 OK\n",
            "INFO:     221.132.29.208:0 - \"GET //assets/Square-oAGqOwsh.js HTTP/1.1\" 200 OK\n",
            "INFO:     221.132.29.208:0 - \"GET //assets/Example-BQyGztrG.js HTTP/1.1\" 200 OK\n",
            "INFO:     221.132.29.208:0 - \"GET //assets/Index-3XHH97VO.js HTTP/1.1\" 200 OK\n",
            "INFO:     221.132.29.208:0 - \"GET //assets/StreamingBar-BU9S4hA7.js HTTP/1.1\" 200 OK\n",
            "INFO:     221.132.29.208:0 - \"GET //assets/Trim-JQYgj7Jd.js HTTP/1.1\" 200 OK\n",
            "INFO:     221.132.29.208:0 - \"GET //assets/ModifyUpload-DFAMaPCH.js HTTP/1.1\" 200 OK\n",
            "INFO:     221.132.29.208:0 - \"GET //assets/InteractiveAudio-D-dIBzGF.js HTTP/1.1\" 200 OK\n",
            "INFO:     221.132.29.208:0 - \"GET //assets/UploadText-sHYakqun.js HTTP/1.1\" 200 OK\n",
            "INFO:     221.132.29.208:0 - \"GET //assets/hls-CnVhpNcu.js HTTP/1.1\" 200 OK\n",
            "INFO:     221.132.29.208:0 - \"GET //assets/DownloadLink-QIttOhoR.js HTTP/1.1\" 200 OK\n",
            "INFO:     221.132.29.208:0 - \"GET //assets/Index-CptIZeFZ.css HTTP/1.1\" 200 OK\n",
            "INFO:     221.132.29.208:0 - \"GET //assets/Upload-B_zkKtR-.js HTTP/1.1\" 200 OK\n",
            "INFO:     221.132.29.208:0 - \"GET //assets/Textbox-pyOssf2O.js HTTP/1.1\" 200 OK\n",
            "INFO:     221.132.29.208:0 - \"GET //assets/Index-DE1Sah7F.js HTTP/1.1\" 200 OK\n",
            "INFO:     221.132.29.208:0 - \"GET //assets/Example-BoMLuz1A.js HTTP/1.1\" 200 OK\n",
            "INFO:     221.132.29.208:0 - \"GET //assets/Index-D7RZ6ETi.js HTTP/1.1\" 200 OK\n",
            "INFO:     221.132.29.208:0 - \"GET //assets/Index-Bih7e2Ax.js HTTP/1.1\" 200 OK\n",
            "INFO:     221.132.29.208:0 - \"GET //assets/Example-Cx2SdskM.js HTTP/1.1\" 200 OK\n",
            "INFO:     221.132.29.208:0 - \"GET //assets/Download-DVtk-Jv3.js HTTP/1.1\" 200 OK\n",
            "INFO:     221.132.29.208:0 - \"GET //assets/Example-ClKJOMGh.css HTTP/1.1\" 200 OK\n",
            "INFO:     221.132.29.208:0 - \"GET //assets/Textbox-jWD3sCxr.css HTTP/1.1\" 200 OK\n",
            "INFO:     221.132.29.208:0 - \"GET /static/fonts/IBMPlexMono/IBMPlexMono-Regular.woff2 HTTP/1.1\" 200 OK\n",
            "INFO:     221.132.29.208:0 - \"POST /gradio_api/queue/join HTTP/1.1\" 200 OK\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/gradio/blocks.py:1927: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
            "  state[block._id] = block.__class__(**kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:     221.132.29.208:0 - \"POST /gradio_api/upload HTTP/1.1\" 200 OK\n",
            "INFO:     221.132.29.208:0 - \"GET /gradio_api/queue/data?session_hash=iyi9r4lhpj HTTP/1.1\" 200 OK\n",
            "INFO:     221.132.29.208:0 - \"GET /gradio_api/file%3D/tmp/gradio/0d665e5c047bb07937c5a8228ec8219fd6a8ad94078fbe3d1e41dcf75cee91a2/audio.wav HTTP/1.1\" 200 OK\n",
            "INFO:     221.132.29.208:0 - \"GET /gradio_api/file%3D/tmp/gradio/0d665e5c047bb07937c5a8228ec8219fd6a8ad94078fbe3d1e41dcf75cee91a2/audio.wav HTTP/1.1\" 200 OK\n",
            "INFO:     221.132.29.208:0 - \"POST /gradio_api/upload HTTP/1.1\" 200 OK\n",
            "INFO:     221.132.29.208:0 - \"POST /gradio_api/queue/join HTTP/1.1\" 200 OK\n",
            "INFO:     221.132.29.208:0 - \"GET /gradio_api/queue/data?session_hash=iyi9r4lhpj HTTP/1.1\" 200 OK\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/gradio/blocks.py:1927: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
            "  state[block._id] = block.__class__(**kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:     221.132.29.208:0 - \"POST /gradio_api/queue/join HTTP/1.1\" 200 OK\n",
            "INFO:     221.132.29.208:0 - \"GET /gradio_api/queue/data?session_hash=iyi9r4lhpj HTTP/1.1\" 200 OK\n",
            "INFO:     221.132.29.208:0 - \"POST /gradio_api/queue/join HTTP/1.1\" 200 OK\n",
            "INFO:     221.132.29.208:0 - \"GET /gradio_api/queue/data?session_hash=iyi9r4lhpj HTTP/1.1\" 200 OK\n",
            "INFO:     221.132.29.208:0 - \"POST /api/transcribe HTTP/1.1\" 200 OK\n",
            "INFO:     5.165.12.71:0 - \"POST /api/transcribe HTTP/1.1\" 200 OK\n",
            "INFO:     5.165.12.71:0 - \"POST /api/transcribe HTTP/1.1\" 200 OK\n",
            "INFO:     221.132.29.208:0 - \"POST /api/speech-to-answer HTTP/1.1\" 200 OK\n"
          ]
        }
      ],
      "source": [
        "# Cell 15 (Updated with Whisper and Streaming)\n",
        "\n",
        "import asyncio\n",
        "import nest_asyncio\n",
        "import uvicorn\n",
        "import os\n",
        "import subprocess\n",
        "import re\n",
        "import time\n",
        "from pydantic import BaseModel\n",
        "from fastapi import FastAPI, HTTPException, UploadFile, File, BackgroundTasks\n",
        "from fastapi.responses import StreamingResponse\n",
        "import gradio as gr\n",
        "from sse_starlette.sse import EventSourceResponse\n",
        "import logging\n",
        "\n",
        "# --- API Models ---\n",
        "class QuestionRequest(BaseModel):\n",
        "    question: str\n",
        "\n",
        "class AnswerResponse(BaseModel):\n",
        "    answer: str\n",
        "\n",
        "# --- 1. Load Model and Initialize RAG (as before) ---\n",
        "print(\"Загрузка модели...\")\n",
        "model, mode = load_model_with_params(CURRENT_CONTEXT_SIZE, CURRENT_GPU_LAYERS)\n",
        "print(f\"Модель загружена. Режим: {mode}\")\n",
        "# Убедитесь, что RAG инициализирован (vector_db существует)\n",
        "\n",
        "# Load Whisper model\n",
        "whisper_model = load_whisper_model()\n",
        "\n",
        "# --- 2. Define Gradio UI (Chat + Admin Panel + Whisper) ---\n",
        "print(\"Определение интерфейса Gradio (Чат + Админ-панель + Whisper)...\")\n",
        "\n",
        "# Whisper processing functions\n",
        "def process_audio_and_fill_chat(audio):\n",
        "    \"\"\"Process audio with Whisper and just fill the chat input\"\"\"\n",
        "    transcription = transcribe_audio(audio)\n",
        "    return transcription, \"\"  # Return to fill input field\n",
        "\n",
        "def process_audio_and_answer(audio):\n",
        "    \"\"\"Process audio with Whisper and directly generate an answer\"\"\"\n",
        "    transcription = transcribe_audio(audio)\n",
        "    # Generate complete answer (non-streaming)\n",
        "    answer = ask_question_with_rag(transcription)\n",
        "    return transcription, answer\n",
        "\n",
        "# Streaming chat handler\n",
        "def submit_message_streaming(message, history):\n",
        "    history.append((message, \"\"))\n",
        "    response_parts = []\n",
        "\n",
        "    for text in ask_question_with_rag_stream(message):\n",
        "        response_parts.append(text)\n",
        "        history[-1] = (message, \"\".join(response_parts))\n",
        "        yield history\n",
        "\n",
        "with gr.Blocks() as demo:\n",
        "    # --- Чат с поддержкой стриминга ---\n",
        "    demo.queue()\n",
        "\n",
        "    with gr.Row():\n",
        "        gr.Markdown(\"### Чат с AI\")\n",
        "\n",
        "    chatbot = gr.Chatbot(label=\"История чата\", height=400)\n",
        "    chat_input = gr.Textbox(label=\"Введите вопрос\", lines=2)\n",
        "\n",
        "    with gr.Row():\n",
        "        submit_button = gr.Button(\"Отправить\")\n",
        "        clear_button = gr.Button(\"Очистить чат\")\n",
        "\n",
        "    # --- Whisper Audio Input ---\n",
        "    with gr.Row():\n",
        "        gr.Markdown(\"### Голосовой ввод\")\n",
        "\n",
        "    with gr.Row():\n",
        "        # Изменяем аудио компонент для поддержки микрофона И загрузки файла\n",
        "        audio_input = gr.Audio(\n",
        "            sources=[\"microphone\", \"upload\"],  # Добавляем возможность загрузки файла\n",
        "            type=\"filepath\",\n",
        "            label=\"Запишите или загрузите голосовой вопрос\",\n",
        "            interactive=True\n",
        "        )\n",
        "        whisper_mode = gr.Radio(\n",
        "            [\"Заполнить поле ввода\", \"Сразу получить ответ\"],\n",
        "            label=\"Режим обработки голоса\",\n",
        "            value=\"Заполнить поле ввода\"\n",
        "        )\n",
        "\n",
        "    with gr.Row():\n",
        "        transcribe_button = gr.Button(\"Обработать аудио\", variant=\"primary\")  # Добавляем variant для визуального выделения\n",
        "\n",
        "    # --- Admin Panel ---\n",
        "    with gr.Row():\n",
        "        gr.Markdown(\"### Админ-панель\")\n",
        "\n",
        "    with gr.Row():\n",
        "        # Model update column\n",
        "        with gr.Column():\n",
        "            model_link = gr.Textbox(label=\"Ссылка на модель (GGUF)\")\n",
        "            update_model_button = gr.Button(\"Обновить модель\")\n",
        "            update_model_output = gr.Textbox(label=\"Статус обновления модели\", interactive=False)\n",
        "            # Bind model update button\n",
        "            update_model_button.click(update_model, inputs=model_link, outputs=update_model_output)\n",
        "\n",
        "        # Context size column\n",
        "        with gr.Column():\n",
        "            context_size_input = gr.Textbox(label=\"Размер контекста (512-8192)\", value=str(CURRENT_CONTEXT_SIZE))\n",
        "            context_size_button = gr.Button(\"Изменить размер контекста\")\n",
        "            context_size_output = gr.Textbox(label=\"Статус изменения контекста\", interactive=False)\n",
        "            # Bind context size button\n",
        "            context_size_button.click(change_context_size, inputs=context_size_input, outputs=context_size_output)\n",
        "\n",
        "    with gr.Row():\n",
        "        # Temperature column\n",
        "        with gr.Column():\n",
        "            temperature_input = gr.Textbox(\n",
        "                label=\"Температура (0.0-2.0)\",\n",
        "                value=str(CURRENT_TEMPERATURE)\n",
        "            )\n",
        "            temperature_button = gr.Button(\"Изменить температуру\")\n",
        "            temperature_output = gr.Textbox(label=\"Статус изменения температуры\", interactive=False)\n",
        "            # Bind temperature button\n",
        "            temperature_button.click(change_temperature, inputs=temperature_input, outputs=temperature_output)\n",
        "\n",
        "        # RAG column\n",
        "        with gr.Column():\n",
        "            rag_button = gr.Button(\"Очистить и переинициализировать RAG\")\n",
        "            rag_output = gr.Textbox(label=\"Статус RAG\", interactive=False)\n",
        "            # Bind RAG button\n",
        "            rag_button.click(clear_rag, outputs=rag_output)\n",
        "\n",
        "    # --- Whisper settings ---\n",
        "    with gr.Row():\n",
        "        with gr.Column():\n",
        "            whisper_size = gr.Radio(\n",
        "                [\"tiny\", \"base\", \"small\", \"medium\", \"large\"],\n",
        "                label=\"Размер модели Whisper\",\n",
        "                value=\"base\"\n",
        "            )\n",
        "            whisper_update_button = gr.Button(\"Обновить модель Whisper\")\n",
        "            whisper_status = gr.Textbox(label=\"Статус Whisper\", interactive=False)\n",
        "\n",
        "            # Function to update Whisper model\n",
        "            def update_whisper_model(size):\n",
        "                try:\n",
        "                    global whisper_model\n",
        "                    whisper_model = load_whisper_model(size)\n",
        "                    return f\"Модель Whisper {size} успешно загружена\"\n",
        "                except Exception as e:\n",
        "                    return f\"Ошибка загрузки модели Whisper: {str(e)}\"\n",
        "\n",
        "            # Bind Whisper update button\n",
        "            whisper_update_button.click(update_whisper_model, inputs=whisper_size, outputs=whisper_status)\n",
        "\n",
        "    # Connect events\n",
        "    submit_button.click(submit_message_streaming, [chat_input, chatbot], [chatbot])\n",
        "    clear_button.click(lambda: ([], \"\"), outputs=[chatbot, chat_input])\n",
        "\n",
        "    # Connect Whisper processing to appropriate function based on mode\n",
        "    def process_audio_with_mode(audio, mode):\n",
        "        if mode == \"Заполнить поле ввода\":\n",
        "            transcription, _ = process_audio_and_fill_chat(audio)  # Правильно распаковываем результат\n",
        "            return transcription, chatbot\n",
        "        else:  # \"Сразу получить ответ\"\n",
        "            transcription = transcribe_audio(audio)\n",
        "            history = [(transcription, \"\")]\n",
        "\n",
        "            # Handle streaming generation\n",
        "            for h in submit_message_streaming(transcription, history):\n",
        "                history = h\n",
        "\n",
        "            return \"\", history\n",
        "\n",
        "    transcribe_button.click(\n",
        "        process_audio_with_mode,\n",
        "        inputs=[audio_input, whisper_mode],\n",
        "        outputs=[chat_input, chatbot]\n",
        "    )\n",
        "\n",
        "print(\"Интерфейс Gradio определен.\")\n",
        "\n",
        "# --- 3. Create FastAPI App ---\n",
        "print(\"Создание FastAPI приложения...\")\n",
        "api_app = FastAPI(title=\"Text Generation API\", description=\"API для генерации текста с использованием RAG\")\n",
        "print(\"FastAPI приложение создано.\")\n",
        "\n",
        "# --- 4. Define API Endpoints ---\n",
        "# Standard /api/ask endpoint\n",
        "@api_app.post(\"/api/ask\",\n",
        "            response_model=AnswerResponse,\n",
        "            summary=\"Задать вопрос модели\",\n",
        "            description=\"Отправляет вопрос модели и возвращает ответ.\")\n",
        "async def handle_ask_api(request: QuestionRequest):\n",
        "    print(f\"[API Request] /api/ask - Вопрос: {request.question}\")\n",
        "    try:\n",
        "        answer = ask_question_with_rag(request.question)\n",
        "        print(f\"[API Response] /api/ask - Ответ сгенерирован.\")\n",
        "        return AnswerResponse(answer=answer)\n",
        "    except Exception as e:\n",
        "        print(f\"[API Error] /api/ask - Ошибка: {str(e)}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        raise HTTPException(status_code=500, detail=f\"Ошибка генерации ответа: {str(e)}\")\n",
        "\n",
        "# New streaming endpoint\n",
        "from fastapi import FastAPI, Request\n",
        "import asyncio\n",
        "import logging\n",
        "from typing import Generator, AsyncGenerator\n",
        "\n",
        "# Enable more detailed logging\n",
        "logging.basicConfig(level=logging.DEBUG)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "@api_app.post(\"/api/ask/stream\",\n",
        "            summary=\"Упрощённый стриминг через SSE\",\n",
        "            description=\"Максимально простая реализация SSE для FastAPI\")\n",
        "async def handle_ask_stream_sse_simplified(request: QuestionRequest):\n",
        "    logger.info(f\"[API Request] /api/ask/stream - Вопрос: {request.question}\")\n",
        "\n",
        "    # Function to convert synchronous generator to async generator\n",
        "    async def async_generator_wrapper(sync_gen: Generator) -> AsyncGenerator:\n",
        "        for item in sync_gen:\n",
        "            # Format SSE message\n",
        "            formatted_item = f\"event: token\\ndata: {item}\\n\\n\"\n",
        "            yield formatted_item\n",
        "            # Small delay\n",
        "            await asyncio.sleep(0.01)\n",
        "        # Send done event at the end\n",
        "        yield \"event: done\\ndata: \\n\\n\"\n",
        "\n",
        "    # Try to get tokens from the model\n",
        "    try:\n",
        "        # Send initial comment to establish connection\n",
        "        async def stream_generator():\n",
        "            # Send initial empty data\n",
        "            yield \"data: \\n\\n\"\n",
        "\n",
        "            # Stream the tokens through the wrapper\n",
        "            async for chunk in async_generator_wrapper(ask_question_with_rag_stream(request.question)):\n",
        "                yield chunk\n",
        "\n",
        "    except Exception as e:\n",
        "        error_message = str(e)\n",
        "        logger.error(f\"Error in SSE generation: {error_message}\", exc_info=True)\n",
        "\n",
        "        # Return an error response\n",
        "        async def error_generator():\n",
        "            yield f\"data: \\n\\n\"\n",
        "            yield f\"event: error\\ndata: Ошибка: {error_message}\\n\\n\"\n",
        "\n",
        "        return StreamingResponse(\n",
        "            error_generator(),\n",
        "            media_type=\"text/event-stream\",\n",
        "            headers={\n",
        "                \"Cache-Control\": \"no-cache, no-transform\",\n",
        "                \"Connection\": \"keep-alive\",\n",
        "                \"X-Accel-Buffering\": \"no\"\n",
        "            }\n",
        "        )\n",
        "\n",
        "    # Return the successful response\n",
        "    return StreamingResponse(\n",
        "        stream_generator(),\n",
        "        media_type=\"text/event-stream\",\n",
        "        headers={\n",
        "            \"Cache-Control\": \"no-cache, no-transform\",\n",
        "            \"Connection\": \"keep-alive\",\n",
        "            \"X-Accel-Buffering\": \"no\"\n",
        "        }\n",
        "    )\n",
        "\n",
        "# New Whisper endpoint\n",
        "@api_app.post(\"/api/transcribe\",\n",
        "            summary=\"Транскрибировать аудио\",\n",
        "            description=\"Преобразует аудио в текст с помощью Whisper.\")\n",
        "async def transcribe_audio_api(file: UploadFile = File(...)):\n",
        "    try:\n",
        "        # Save the uploaded file temporarily\n",
        "        temp_file = tempfile.NamedTemporaryFile(delete=False, suffix=\".wav\")\n",
        "        temp_file.close()\n",
        "\n",
        "        # Write uploaded file content\n",
        "        with open(temp_file.name, \"wb\") as f:\n",
        "            f.write(await file.read())\n",
        "\n",
        "        # Transcribe the audio\n",
        "        transcribed_text = transcribe_audio(temp_file.name)\n",
        "\n",
        "        # Clean up\n",
        "        os.unlink(temp_file.name)\n",
        "\n",
        "        return {\"text\": transcribed_text}\n",
        "    except Exception as e:\n",
        "        raise HTTPException(status_code=500, detail=f\"Ошибка транскрибирования: {str(e)}\")\n",
        "\n",
        "# Combined endpoint: transcribe + generate answer\n",
        "@api_app.post(\"/api/speech-to-answer\",\n",
        "            summary=\"Получить ответ на голосовой вопрос\",\n",
        "            description=\"Преобразует аудио в текст и генерирует ответ.\")\n",
        "async def speech_to_answer_api(file: UploadFile = File(...)):\n",
        "    try:\n",
        "        # Save the uploaded file temporarily\n",
        "        temp_file = tempfile.NamedTemporaryFile(delete=False, suffix=\".wav\")\n",
        "        temp_file.close()\n",
        "\n",
        "        # Write uploaded file content\n",
        "        with open(temp_file.name, \"wb\") as f:\n",
        "            f.write(await file.read())\n",
        "\n",
        "        # Transcribe the audio\n",
        "        transcribed_text = transcribe_audio(temp_file.name)\n",
        "\n",
        "        # Clean up\n",
        "        os.unlink(temp_file.name)\n",
        "\n",
        "        # Generate answer\n",
        "        answer = ask_question_with_rag(transcribed_text)\n",
        "\n",
        "        return {\"transcription\": transcribed_text, \"answer\": answer}\n",
        "    except Exception as e:\n",
        "        raise HTTPException(status_code=500, detail=f\"Ошибка обработки речи: {str(e)}\")\n",
        "\n",
        "# --- 5. Mount Gradio App onto FastAPI ---\n",
        "print(\"Монтирование Gradio UI на FastAPI...\")\n",
        "api_app = gr.mount_gradio_app(api_app, demo, path=\"/\")\n",
        "print(\"Gradio UI смонтирован на FastAPI по пути '/'.\")\n",
        "\n",
        "# --- 6. Run with Uvicorn and Cloudflared Tunnel ---\n",
        "# Same as in your original code\n",
        "nest_asyncio.apply()\n",
        "\n",
        "async def run_server_and_tunnel():\n",
        "    config = uvicorn.Config(app=api_app, host=\"0.0.0.0\", port=7860, log_level=\"info\")\n",
        "    server = uvicorn.Server(config)\n",
        "    server_task = asyncio.create_task(server.serve())\n",
        "    print(\"Сервер Uvicorn запущен в фоновом режиме.\")\n",
        "    await asyncio.sleep(5)\n",
        "\n",
        "    print(\"Запуск cloudflared...\")\n",
        "    cf_process = subprocess.Popen(\n",
        "        ['cloudflared', 'tunnel', '--url', 'http://localhost:7860'],\n",
        "        stdout=subprocess.PIPE, stderr=subprocess.PIPE, universal_newlines=True\n",
        "    )\n",
        "\n",
        "    public_url = None\n",
        "    login_url_printed = False\n",
        "    start_time = time.time()\n",
        "    try:\n",
        "        while time.time() - start_time < 60:\n",
        "            line = cf_process.stderr.readline()\n",
        "            if not line and cf_process.poll() is not None: break\n",
        "            if not line:\n",
        "                await asyncio.sleep(0.5); continue\n",
        "            print(f\"[cloudflared] {line.strip()}\")\n",
        "\n",
        "            # Логин URL (первый раз)\n",
        "            if \"https://dash.cloudflare.com/argotunnel?callback=\" in line:\n",
        "                 login_url = re.search(r'(https://dash.cloudflare.com/argotunnel\\?callback=[^\\s]+)', line)\n",
        "                 if login_url:\n",
        "                      print(\"\\n\" + \"=\"*50 + \"\\n‼️ ТРЕБУЕТСЯ АУТЕНТИФИКАЦИЯ CLOUDFLARE (ОДИН РАЗ) ‼️\")\n",
        "                      print(f\"1. Скопируйте ссылку:\\n    {login_url.group(1)}\\n\")\n",
        "                      print(\"2. Откройте в браузере, войдите в Cloudflare, авторизуйте.\")\n",
        "                      print(\"3. После успеха в браузере, ОСТАНОВИТЕ и ПЕРЕЗАПУСТИТЕ эту ячейку.\\n\" + \"=\"*50 + \"\\n\")\n",
        "                      login_url_printed = True; cf_process.terminate(); await cf_process.wait(); return\n",
        "\n",
        "            # Готовый URL (после логина)\n",
        "            tunnel_url_match = re.search(r'(https://[a-zA-Z0-9-]+\\.trycloudflare\\.com)', line)\n",
        "            if tunnel_url_match:\n",
        "                public_url = tunnel_url_match.group(1)\n",
        "                print(\"\\n\" + \"=\"*50)\n",
        "                print(f\"✅ Публичный URL (Cloudflare Tunnel): {public_url}\")\n",
        "                print(f\"   API: {public_url}/api/ask | Docs: {public_url}/docs\")\n",
        "                print(\"--- Сервер и туннель активны. Остановите ячейку для завершения. ---\\n\" + \"=\"*50 + \"\\n\")\n",
        "                break # URL найден\n",
        "\n",
        "        if not public_url and not login_url_printed:\n",
        "             print(\"Не удалось получить URL от cloudflared за 60 секунд.\"); cf_process.terminate(); await cf_process.wait(); return\n",
        "\n",
        "        await server_task # Ждем завершения Uvicorn\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Ошибка: {e}\"); server_task.cancel()\n",
        "        try: await server_task\n",
        "        except asyncio.CancelledError: print(\"Сервер Uvicorn остановлен.\")\n",
        "    finally:\n",
        "        print(\"Завершение cloudflared...\");\n",
        "        if cf_process.poll() is None: # Проверяем, жив ли еще процесс\n",
        "             cf_process.terminate()\n",
        "             try:\n",
        "                 await asyncio.wait_for(asyncio.to_thread(cf_process.wait), timeout=5.0)\n",
        "             except asyncio.TimeoutError:\n",
        "                 print(\"Процесс cloudflared не завершился вовремя, убиваем...\")\n",
        "                 cf_process.kill()\n",
        "                 await asyncio.to_thread(cf_process.wait)\n",
        "        print(\"Процесс cloudflared завершен.\")\n",
        "\n",
        "\n",
        "# --- Запуск ---\n",
        "print(\"Запуск асинхронной функции для сервера и туннеля...\")\n",
        "asyncio.run(run_server_and_tunnel())"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}